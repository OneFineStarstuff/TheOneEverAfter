{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQn/pdnadpHA65KYAcXV9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Knowledge_Distillation_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "class DistillationTrainer:\n",
        "    def __init__(self, teacher_model, student_model, temperature=3.0, alpha=0.5, learning_rate=1e-4):\n",
        "        self.teacher = teacher_model\n",
        "        self.student = student_model\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.optimizer = Adam(self.student.parameters(), lr=learning_rate)\n",
        "\n",
        "    def distillation_loss(self, student_logits, teacher_logits, labels):\n",
        "        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
        "        student_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
        "        distillation_loss = F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\") * (self.temperature ** 2)\n",
        "        classification_loss = F.cross_entropy(student_logits, labels)\n",
        "        return self.alpha * distillation_loss + (1 - self.alpha) * classification_loss\n",
        "\n",
        "    def train_step(self, input_data, labels):\n",
        "        self.student.train()\n",
        "        self.teacher.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = self.teacher(**input_data).logits\n",
        "\n",
        "        student_logits = self.student(**input_data).logits\n",
        "        loss = self.distillation_loss(student_logits, teacher_logits, labels)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, dataloader, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch in dataloader:\n",
        "                input_data = {\n",
        "                    \"input_ids\": batch[\"input_ids\"],\n",
        "                    \"attention_mask\": batch[\"attention_mask\"]\n",
        "                }\n",
        "                labels = batch[\"labels\"]\n",
        "                loss = self.train_step(input_data, labels)\n",
        "                total_loss += loss\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Helper function to pad sequences\n",
        "def pad_sequences(tokenizer, texts, max_length):\n",
        "    return tokenizer(\n",
        "        texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    teacher_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    student_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Create a list of texts and labels\n",
        "    texts = [\"Sample text for training\", \"Another sample text\"]\n",
        "    labels = torch.tensor([1, 0])\n",
        "\n",
        "    # Pad the sequences\n",
        "    max_length = 10  # Define the maximum length for padding\n",
        "    padded_inputs = pad_sequences(tokenizer, texts, max_length)\n",
        "\n",
        "    # Create a DataLoader\n",
        "    data = [{\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label}\n",
        "            for input_ids, attention_mask, label in zip(padded_inputs[\"input_ids\"], padded_inputs[\"attention_mask\"], labels)]\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(data, batch_size=2)\n",
        "\n",
        "    epochs = 10\n",
        "\n",
        "    trainer = DistillationTrainer(teacher_model, student_model, temperature=3.0, alpha=0.5, learning_rate=1e-4)\n",
        "    trainer.train(dataloader, epochs)"
      ],
      "metadata": {
        "id": "7I8G5hABOuWX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}