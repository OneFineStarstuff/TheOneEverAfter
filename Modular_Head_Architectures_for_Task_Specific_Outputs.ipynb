{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOoxm+8uAtfroO/ZLiRdbrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Modular_Head_Architectures_for_Task_Specific_Outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random  # Add this import statement\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "# Download NLTK wordnet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the ModularFoundationModel class\n",
        "class ModularFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes=2):\n",
        "        super(ModularFoundationModel, self).__init__()\n",
        "        self.core_model = BertModel.from_pretrained(model_name)\n",
        "        self.classification_head = nn.Linear(self.core_model.config.hidden_size, num_classes)\n",
        "        self.qa_head = nn.Linear(self.core_model.config.hidden_size, 2)  # Start and end logits\n",
        "        self.summarization_head = nn.Linear(self.core_model.config.hidden_size, self.core_model.config.vocab_size)  # Summarization\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task_type):\n",
        "        core_outputs = self.core_model(input_ids, attention_mask=attention_mask)\n",
        "        if task_type == \"classification\":\n",
        "            return self.classification_head(core_outputs.last_hidden_state[:, 0, :])  # [CLS] token\n",
        "        elif task_type == \"question_answering\":\n",
        "            return self.qa_head(core_outputs.last_hidden_state)\n",
        "        elif task_type == \"summarization\":\n",
        "            return self.summarization_head(core_outputs.last_hidden_state)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported task type: {}\".format(task_type))\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)  # Ensure random module is used here\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Augmenting the dataset with more examples and synonym replacement\n",
        "texts = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"label\": 0},\n",
        "    {\"text\": \"A journey of a thousand miles begins with a single step.\", \"label\": 0},\n",
        "    {\"text\": \"To be or not to be, that is the question.\", \"label\": 0},\n",
        "    {\"text\": \"All that glitters is not gold.\", \"label\": 0},\n",
        "    {\"text\": \"The early bird catches the worm.\", \"label\": 1},\n",
        "    {\"text\": \"A picture is worth a thousand words.\", \"label\": 1},\n",
        "    {\"text\": \"Better late than never.\", \"label\": 1},\n",
        "    {\"text\": \"Actions speak louder than words.\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Augmenting data with synonyms\n",
        "augmented_texts = []\n",
        "for text in texts:\n",
        "    for _ in range(3):  # Create 3 augmented versions of each sentence\n",
        "        augmented_text = synonym_replacement(text[\"text\"])\n",
        "        augmented_texts.append({\"text\": augmented_text, \"label\": text[\"label\"]})\n",
        "texts.extend(augmented_texts)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(texts)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data, tokenizer, for_classification=True)\n",
        "val_dataset = TextDataset(val_data, tokenizer, for_classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = ModularFoundationModel(model_name=\"bert-base-uncased\", num_classes=2).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Define the prompt generation function\n",
        "def generate_prompt(text, task_description):\n",
        "    prompt = f\"{task_description}: {text}\"\n",
        "    return tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Define your datasets for each task\n",
        "summarization_data = [\n",
        "    {\"text\": \"Example summarization text.\", \"label\": 0},\n",
        "    # Add your summarization data here\n",
        "]\n",
        "\n",
        "qa_data = [\n",
        "    {\"text\": \"Example QA text.\", \"label\": [0, 1]},  # Labels for start and end positions\n",
        "    # Add your QA data here\n",
        "]\n",
        "\n",
        "classification_data = [\n",
        "    {\"text\": \"Example classification text.\", \"label\": 0},\n",
        "    # Add your classification data here\n",
        "]\n",
        "\n",
        "# Create datasets for each task\n",
        "summarization_dataset = TextDataset(summarization_data, tokenizer, for_classification=True)\n",
        "qa_dataset = TextDataset(qa_data, tokenizer, for_classification=True)\n",
        "classification_dataset = TextDataset(classification_data, tokenizer, for_classification=True)\n",
        "\n",
        "# Create data loaders for each task\n",
        "summarization_loader = DataLoader(summarization_dataset, batch_size=1, shuffle=True)\n",
        "qa_loader = DataLoader(qa_dataset, batch_size=1, shuffle=True)\n",
        "classification_loader = DataLoader(classification_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop for multi-task learning\n",
        "task_dataloaders = {\n",
        "    \"summarization\": summarization_loader,\n",
        "    \"question_answering\": qa_loader,\n",
        "    \"classification\": classification_loader\n",
        "}\n",
        "\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    for task, dataloader in task_dataloaders.items():\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            for i in range(input_ids.size(0)):\n",
        "                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "                prompted_inputs = generate_prompt(input_text, task)\n",
        "                input_ids = prompted_inputs['input_ids'].to(device)\n",
        "                attention_mask = prompted_inputs['attention_mask'].to(device)\n",
        "\n",
        "                logits = model(input_ids, attention_mask, task_type=task)\n",
        "                if task == \"classification\":\n",
        "                    loss = F.cross_entropy(logits, labels[i].unsqueeze(0))\n",
        "                elif task == \"question_answering\":\n",
        "                    start_logits, end_logits = logits.split(1, dim=-1)\n",
        "                    label_tensor = torch.tensor(labels[i]).to(device)\n",
        "                    if len(label_tensor) != 2:\n",
        "                        print(f\"Invalid label tensor length for QA task: {len(label_tensor)}\")\n",
        "                        continue\n",
        "                    start_loss = F.cross_entropy(start_logits.squeeze(-1), label_tensor[0].unsqueeze(0))\n",
        "                    end_loss = F.cross_entropy(end_logits.squeeze(-1), label_tensor[1].unsqueeze(0))\n",
        "                    loss = (start_loss + end_loss) / 2\n",
        "                elif task == \"summarization\":\n",
        "                    # For summarization, we need to ensure the batch size matches.\n",
        "                    labels_summarization = labels[i].view(-1)\n",
        "                    logits_summarization = logits.view(-1, logits.size(-1))\n",
        "                    if logits_summarization.size(0) != labels_summarization.size(0):\n",
        "                        labels_summarization = labels_summarization.expand(logits_summarization.size(0))\n",
        "                    loss = F.cross_entropy(logits_summarization, labels_summarization)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print(f\"Task: {task}, Epoch: {epoch + 1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "CcRDQWlW7ply"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}