{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOt5UgIwXGcPhL1vRJhs/3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Longformer_Model_Implementation_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aik0NNr5vAwA"
      },
      "outputs": [],
      "source": [
        "pip install fastapi uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from transformers import AdamW as TransformersAdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "# Download NLTK wordnet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cpu\")  # Switch to CPU to reduce memory usage\n",
        "\n",
        "## Dataset Definition\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=64, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "## MAML Model Definition\n",
        "\n",
        "class MAMLModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state[:, 0, :]  # Use CLS token\n",
        "\n",
        "    def clone_parameters(self):\n",
        "        return {name: param.clone() for name, param in self.named_parameters()}\n",
        "\n",
        "    def fast_adapt(self, support_data, query_data, optimizer, n_steps=5, lr_inner=0.01):\n",
        "        original_params = self.clone_parameters()\n",
        "\n",
        "        for _ in range(n_steps):\n",
        "            support_input, support_attention, support_target = support_data\n",
        "            optimizer.zero_grad()\n",
        "            logits = self(support_input, support_attention)\n",
        "            loss = F.cross_entropy(logits, support_target)\n",
        "            loss.backward()\n",
        "\n",
        "            for name, param in self.named_parameters():\n",
        "                if param.grad is not None:  # Check for None gradients\n",
        "                    param.data -= lr_inner * param.grad\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        query_input, query_attention, query_target = query_data\n",
        "        query_logits = self(query_input, query_attention)\n",
        "        query_loss = F.cross_entropy(query_logits, query_target)\n",
        "\n",
        "        # Restore original parameters\n",
        "        for name, param in self.named_parameters():\n",
        "            param.data = original_params[name]\n",
        "\n",
        "        return query_loss\n",
        "\n",
        "## Data Augmentation Function\n",
        "\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Original dataset\n",
        "texts = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"label\": 0},\n",
        "    {\"text\": \"A journey of a thousand miles begins with a single step.\", \"label\": 0},\n",
        "    {\"text\": \"To be or not to be, that is the question.\", \"label\": 0},\n",
        "    {\"text\": \"All that glitters is not gold.\", \"label\": 0},\n",
        "    {\"text\": \"The early bird catches the worm.\", \"label\": 1},\n",
        "    {\"text\": \"A picture is worth a thousand words.\", \"label\": 1},\n",
        "    {\"text\": \"Better late than never.\", \"label\": 1},\n",
        "    {\"text\": \"Actions speak louder than words.\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Augmenting data with synonyms\n",
        "augmented_texts = []\n",
        "for text in texts:\n",
        "    for _ in range(3):  # Create 3 augmented versions of each sentence\n",
        "        augmented_text = synonym_replacement(text[\"text\"])\n",
        "        augmented_texts.append({\"text\": augmented_text, \"label\": text[\"label\"]})\n",
        "\n",
        "texts.extend(augmented_texts)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(texts)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data, tokenizer, for_classification=True)\n",
        "val_dataset = TextDataset(val_data, tokenizer, for_classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "## Model Initialization\n",
        "\n",
        "# Initialize Longformer model and MAML model wrapper\n",
        "base_model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "maml_model = MAMLModel(base_model).to(device)\n",
        "optimizer = TransformersAdamW(maml_model.parameters(), lr=5e-5)\n",
        "\n",
        "## Training Loop with Debugging Statements\n",
        "\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    for support_batch, query_batch in zip(train_dataloader, val_dataloader):\n",
        "        support_input, support_attention, support_target = support_batch\n",
        "        query_input, query_attention, query_target = query_batch\n",
        "\n",
        "        # Move tensors to device and check shapes before passing to fast_adapt\n",
        "        support_input = support_input.to(device)\n",
        "        support_attention = support_attention.to(device)\n",
        "        support_target = support_target.to(device)\n",
        "        query_input = query_input.to(device)\n",
        "        query_attention = query_attention.to(device)\n",
        "        query_target = query_target.to(device)\n",
        "\n",
        "        print(f\"Support Input Shape: {support_input.shape}\")\n",
        "        print(f\"Support Attention Shape: {support_attention.shape}\")\n",
        "        print(f\"Support Target Shape: {support_target.shape}\")\n",
        "\n",
        "        print(f\"Query Input Shape: {query_input.shape}\")\n",
        "        print(f\"Query Attention Shape: {query_attention.shape}\")\n",
        "        print(f\"Query Target Shape: {query_target.shape}\")\n",
        "\n",
        "        support_data = (support_input, support_attention, support_target)\n",
        "        query_data = (query_input, query_attention, query_target)\n",
        "\n",
        "        # Call fast_adapt and print loss information\n",
        "        query_loss = maml_model.fast_adapt(support_data, query_data, optimizer)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Query Loss: {query_loss.item()}\")\n",
        "\n",
        "## FastAPI Application Setup\n",
        "\n",
        "from fastapi import FastAPI, Request\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load trained model and tokenizer (assuming they are defined elsewhere)\n",
        "maml_model.eval()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Request):\n",
        "    data = await request.json()\n",
        "    text = data[\"text\"]\n",
        "\n",
        "    # Tokenize input and move to device\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=2048, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Predict using the model's forward method based on the task type (classification)\n",
        "    logits = maml_model(input_ids, attention_mask)\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    return {\"predictions\": predictions.tolist()}\n",
        "\n",
        "# Run server command: uvicorn script_name:app --reload"
      ],
      "metadata": {
        "id": "HYOa1SDCvCuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}