{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOd9Lwovhd+ekgk0KDFc/KZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Longformer_and_CNN_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi"
      ],
      "metadata": {
        "id": "m9iTz48wAxtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AutoModel, AdamW\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from fastapi import FastAPI, Request\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import logging\n",
        "\n",
        "# --- Configuration ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# --- Text Dataset ---\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=2048, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# --- Perception Module ---\n",
        "class PerceptionModule(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, sensor_dim, hidden_dim, text_model=\"allenai/longformer-base-4096\"):\n",
        "        super(PerceptionModule, self).__init__()\n",
        "        # Text model (Longformer)\n",
        "        self.text_model = LongformerModel.from_pretrained(text_model)\n",
        "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, hidden_dim)\n",
        "\n",
        "        # Image CNN\n",
        "        self.image_cnn = nn.Sequential(\n",
        "            nn.Conv2d(image_dim[0], 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.image_fc = nn.Linear(32 * (image_dim[1] // 4) * (image_dim[2] // 4), hidden_dim)\n",
        "\n",
        "        # Sensor data processing\n",
        "        self.sensor_fc = nn.Linear(sensor_dim, hidden_dim)\n",
        "\n",
        "        # Combined feature layer\n",
        "        self.fc = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "\n",
        "    def forward(self, text, image, sensor):\n",
        "        text_features = F.relu(self.text_fc(self.text_model(**text).last_hidden_state.mean(dim=1)))\n",
        "        image_features = F.relu(self.image_fc(self.image_cnn(image)))\n",
        "        sensor_features = F.relu(self.sensor_fc(sensor))\n",
        "        combined_features = torch.cat((text_features, image_features, sensor_features), dim=1)\n",
        "        return F.relu(self.fc(combined_features))\n",
        "\n",
        "# --- Memory Module ---\n",
        "class MemoryBank(nn.Module):\n",
        "    def __init__(self, memory_size, memory_dim):\n",
        "        super(MemoryBank, self).__init__()\n",
        "        self.keys = torch.randn(memory_size, memory_dim).to(device)\n",
        "        self.values = torch.randn(memory_size, memory_dim).to(device)\n",
        "        self.access_count = torch.zeros(memory_size).to(device)\n",
        "\n",
        "    def write(self, key, value):\n",
        "        idx = torch.argmin(self.access_count)\n",
        "        self.keys[idx] = key\n",
        "        self.values[idx] = value\n",
        "        self.access_count[idx] = 0\n",
        "\n",
        "    def read(self, key):\n",
        "        idx = torch.argmax(torch.cosine_similarity(self.keys, key.unsqueeze(0)))\n",
        "        self.access_count[idx] += 1\n",
        "        return self.values[idx]\n",
        "\n",
        "# --- Decision Making Module ---\n",
        "class DecisionMakingModule(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DecisionMakingModule, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.fc(features)\n",
        "\n",
        "# --- Unified AGI System ---\n",
        "class UnifiedAGISystem(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, sensor_dim, hidden_dim, memory_size, output_dim):\n",
        "        super(UnifiedAGISystem, self).__init__()\n",
        "        self.perception = PerceptionModule(text_dim, image_dim, sensor_dim, hidden_dim)\n",
        "        self.memory = MemoryBank(memory_size, hidden_dim)\n",
        "        self.decision_making = DecisionMakingModule(hidden_dim, output_dim)\n",
        "\n",
        "    def perform_task(self, text, image, sensor):\n",
        "        features = self.perception(text, image, sensor)\n",
        "        self.memory.write(features, features)\n",
        "        decision = self.decision_making(features)\n",
        "        return decision\n",
        "\n",
        "# --- Training Function ---\n",
        "def train(model, train_loader, criterion, optimizer, epochs=10, use_amp=True):\n",
        "    scaler = GradScaler(enabled=use_amp)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in tqdm(train_loader):\n",
        "            images, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            with autocast(enabled=use_amp):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            running_loss += loss.item()\n",
        "        logging.info(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# --- Deployment with FastAPI ---\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Request):\n",
        "    data = await request.json()\n",
        "    text = data[\"text\"]\n",
        "    image = data[\"image\"]\n",
        "    sensor = torch.tensor(data[\"sensor\"]).float().to(device)\n",
        "\n",
        "    # Tokenize and preprocess inputs\n",
        "    text_inputs = model.text_model.tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    image_tensor = torch.tensor(image).unsqueeze(0).to(device)\n",
        "\n",
        "    decision = agi_system.perform_task(text_inputs, image_tensor, sensor)\n",
        "    return {\"decision\": decision.tolist()}\n",
        "\n",
        "# To run the server, execute: uvicorn script_name:app --reload"
      ],
      "metadata": {
        "id": "XCcTyKwOBsPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}