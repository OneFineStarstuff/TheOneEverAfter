{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOO3LtIegcfDRXWy0eip7oQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Distributed_Training_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
        "\n",
        "# Define a simple model for demonstration\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    # Initialize process group\n",
        "    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def train(rank, world_size):\n",
        "    setup(rank, world_size)\n",
        "\n",
        "    # Set up device and model\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "    model = SimpleModel().to(device)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    # Create dummy dataset and DataLoader\n",
        "    dataset = TensorDataset(torch.randn(100, 10), torch.randint(0, 2, (100,)))\n",
        "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
        "    dataloader = DataLoader(dataset, batch_size=16, sampler=sampler)\n",
        "\n",
        "    # Set up optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Number of epochs\n",
        "        sampler.set_epoch(epoch)\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if rank == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    cleanup()\n",
        "\n",
        "def main():\n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JR5M1ZPaeA6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}