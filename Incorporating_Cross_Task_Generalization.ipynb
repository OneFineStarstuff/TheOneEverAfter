{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNrRXCuS7G2ELo7l1miYfB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Incorporating_Cross_Task_Generalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlmDJtXPuIKe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from transformers import AdamW as TransformersAdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "# Download NLTK wordnet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cpu\")  # Switch to CPU to reduce memory usage\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=64, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define the MAML model class\n",
        "class MAMLModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state[:, 0, :]  # Use CLS token\n",
        "\n",
        "    def clone_parameters(self):\n",
        "        return {name: param.clone() for name, param in self.named_parameters()}\n",
        "\n",
        "    def fast_adapt(self, support_data, query_data, optimizer, n_steps=5, lr_inner=0.01):\n",
        "        original_params = self.clone_parameters()\n",
        "        for _ in range(n_steps):\n",
        "            support_input, support_attention, support_target = support_data\n",
        "            optimizer.zero_grad()\n",
        "            logits = self(support_input, support_attention)\n",
        "            loss = F.cross_entropy(logits, support_target)\n",
        "            loss.backward()\n",
        "\n",
        "            for name, param in self.named_parameters():\n",
        "                if param.grad is not None:  # Check for None gradients\n",
        "                    param.data -= lr_inner * param.grad\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        query_input, query_attention, query_target = query_data\n",
        "        query_logits = self(query_input, query_attention)\n",
        "        query_loss = F.cross_entropy(query_logits, query_target)\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            param.data = original_params[name]  # Restore original parameters\n",
        "\n",
        "        return query_loss\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Augmenting the dataset with more examples and synonym replacement\n",
        "texts = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"label\": 0},\n",
        "    {\"text\": \"A journey of a thousand miles begins with a single step.\", \"label\": 0},\n",
        "    {\"text\": \"To be or not to be, that is the question.\", \"label\": 0},\n",
        "    {\"text\": \"All that glitters is not gold.\", \"label\": 0},\n",
        "    {\"text\": \"The early bird catches the worm.\", \"label\": 1},\n",
        "    {\"text\": \"A picture is worth a thousand words.\", \"label\": 1},\n",
        "    {\"text\": \"Better late than never.\", \"label\": 1},\n",
        "    {\"text\": \"Actions speak louder than words.\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Augmenting data with synonyms\n",
        "augmented_texts = []\n",
        "for text in texts:\n",
        "    for _ in range(3):  # Create 3 augmented versions of each sentence\n",
        "        augmented_text = synonym_replacement(text[\"text\"])\n",
        "        augmented_texts.append({\"text\": augmented_text, \"label\": text[\"label\"]})\n",
        "texts.extend(augmented_texts)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(texts)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data, tokenizer, for_classification=True)\n",
        "val_dataset = TextDataset(val_data, tokenizer, for_classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Initialize Longformer model\n",
        "base_model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "maml_model = MAMLModel(base_model).to(device)\n",
        "optimizer = TransformersAdamW(maml_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Define the prompt generation function\n",
        "def generate_prompt(text, task_description):\n",
        "    prompt = f\"{task_description}: {text}\"\n",
        "    return tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Define your datasets for each task\n",
        "summarization_data = [\n",
        "    {\"text\": \"Example summarization text.\", \"label\": 0},\n",
        "    # Add your summarization data here\n",
        "]\n",
        "\n",
        "qa_data = [\n",
        "    {\"text\": \"Example QA text.\", \"label\": 0},\n",
        "    # Add your QA data here\n",
        "]\n",
        "\n",
        "classification_data = [\n",
        "    {\"text\": \"Example classification text.\", \"label\": 0},\n",
        "    # Add your classification data here\n",
        "]\n",
        "\n",
        "# Create datasets for each task\n",
        "summarization_dataset = TextDataset(summarization_data, tokenizer, for_classification=True)\n",
        "qa_dataset = TextDataset(qa_data, tokenizer, for_classification=True)\n",
        "classification_dataset = TextDataset(classification_data, tokenizer, for_classification=True)\n",
        "\n",
        "# Create data loaders for each task\n",
        "summarization_loader = DataLoader(summarization_dataset, batch_size=1, shuffle=True)\n",
        "qa_loader = DataLoader(qa_dataset, batch_size=1, shuffle=True)\n",
        "classification_loader = DataLoader(classification_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop for multi-task learning\n",
        "task_dataloaders = {\n",
        "    \"summarization\": summarization_loader,\n",
        "    \"question_answering\": qa_loader,\n",
        "    \"classification\": classification_loader\n",
        "}\n",
        "\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    for task, dataloader in task_dataloaders.items():\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            # Unpack the batch correctly\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            for i in range(input_ids.size(0)):\n",
        "                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "                prompted_inputs = generate_prompt(input_text, task)\n",
        "                input_ids = prompted_inputs['input_ids'].to(device)\n",
        "                attention_mask = prompted_inputs['attention_mask'].to(device)\n",
        "\n",
        "                logits = maml_model(input_ids, attention_mask)\n",
        "                loss = F.cross_entropy(logits, labels[i].unsqueeze(0))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print(f\"Task: {task}, Epoch: {epoch + 1}, Loss: {loss.item()}\")"
      ]
    }
  ]
}