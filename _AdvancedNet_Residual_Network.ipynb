{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNLUMLQCdt7fzbu74IdI8mQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/_AdvancedNet_Residual_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "PXmk87BXn9XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install diffprivlib"
      ],
      "metadata": {
        "id": "RXi9Xo2QoNw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install aif360"
      ],
      "metadata": {
        "id": "DpwDgiIDoUHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reco-gym\n",
        "!pip install 'aif360[Reductions]'"
      ],
      "metadata": {
        "id": "yWepUHqmodL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "geHc90djpDkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjKVwgQunv4u"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import Libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import logging\n",
        "import requests\n",
        "import optuna\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy import stats\n",
        "from albumentations import Compose, RandomRotate90, Flip, Transpose, RandomBrightnessContrast\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from diffprivlib.models import LogisticRegression as DPLogisticRegression\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from reco_gym import build_agent_init, test_agent, env_1_args\n",
        "from reco_gym.agent import random_agent_args, critical_agent_args\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n",
        "\n",
        "# Define AdvancedNet with Residual Connections\n",
        "class AdvancedNet(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(AdvancedNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc_residual = nn.Linear(784, 128)\n",
        "        self.fc_out = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x2 = self.dropout(torch.relu(self.bn2(self.fc2(x1))))\n",
        "        x_residual = self.fc_residual(x)\n",
        "        x2 += x_residual\n",
        "        output = self.fc_out(x2)\n",
        "        return output\n",
        "\n",
        "# Data Setup\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Model, Optimizer, Criterion\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AdvancedNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# Training Loop with Mixed Precision and Gradient Accumulation\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "num_epochs = 10\n",
        "accumulation_steps = 4\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.view(-1, 784).to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels) / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Save Model Checkpoints\n",
        "def save_checkpoint(model, optimizer, epoch, version=\"1.0\"):\n",
        "    checkpoint_dir = f\"checkpoints/v{version}\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "# Model Evaluation with Metrics\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.view(-1, 784).to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Hyperparameter Tuning with Optuna\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
        "    model = AdvancedNet(dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    val_loss = evaluate_model(model, DataLoader(datasets.MNIST(root='./data', train=False, download=True, transform=transform)))\n",
        "    return val_loss\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "# Logging\n",
        "writer = SummaryWriter(log_dir='runs/experiment_1')\n",
        "for epoch in range(num_epochs):\n",
        "    writer.add_scalar('Loss/train', running_loss / len(train_loader), epoch)\n",
        "writer.close()\n",
        "\n",
        "# Slack Notification\n",
        "def send_slack_notification(message, webhook_url):\n",
        "    payload = {\"text\": message}\n",
        "    response = requests.post(webhook_url, json=payload)\n",
        "    return response.status_code\n",
        "\n",
        "webhook_url = \"YOUR_SLACK_WEBHOOK_URL\"\n",
        "message = \"Drift detected in the model. Immediate action required.\"\n",
        "send_slack_notification(message, webhook_url)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, filename='model_logs.log', filemode='a', format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def log_model_event(event):\n",
        "    logging.info(event)\n",
        "\n",
        "log_model_event(\"Model training started.\")\n",
        "log_model_event(\"Model training completed.\")\n",
        "\n",
        "# Tracing setup\n",
        "trace.set_tracer_provider(TracerProvider())\n",
        "span_processor = BatchSpanProcessor(ConsoleSpanExporter())\n",
        "trace.get_tracer_provider().add_span_processor(span_processor)\n",
        "\n",
        "tracer = trace.get_tracer(__name__)\n",
        "\n",
        "with tracer.start_as_current_span(\"main\"):\n",
        "    with tracer.start_as_current_span(\"data_loading\"):\n",
        "        # Data loading code\n",
        "        pass\n",
        "\n",
        "    with tracer.start_as_current_span(\"model_inference\"):\n",
        "        # Model inference code\n",
        "        pass\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train base models\n",
        "rf = RandomForestClassifier()\n",
        "gb = GradientBoostingClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Stack base model predictions\n",
        "meta_features = np.column_stack((rf.predict(X_test), gb.predict(X_test)))\n",
        "\n",
        "# Train meta-model\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_features, y_test)\n",
        "\n",
        "# Evaluate stacked model\n",
        "meta_predictions = meta_model.predict(meta_features)\n",
        "print(f\"Stacked model accuracy: {accuracy_score(y_test, meta_predictions)}\")\n",
        "\n",
        "# Data Augmentation\n",
        "augmentor = Compose([\n",
        "    RandomRotate90(),\n",
        "    Flip(),\n",
        "    Transpose(),\n",
        "    RandomBrightnessContrast(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def augment_image(image):\n",
        "    augmented = augmentor(image=image)\n",
        "    return augmented['image']\n",
        "\n",
        "# Example usage of augmented image\n",
        "image = ...  # Replace with actual image data\n",
        "augmented_image = augment_image(image)\n",
        "\n",
        "# Differential Privacy\n",
        "dp_model = DPLogisticRegression(epsilon=1.0)\n",
        "dp_model.fit(X_train, y_train)\n",
        "predictions = dp_model.predict(X_test)\n",
        "\n",
        "# Fairness Metrics\n",
        "df = ...  # Your dataframe here\n",
        "dataset = BinaryLabelDataset(df=df, label_names=['label'], protected_attribute_names=['gender'])\n",
        "metric = BinaryLabelDatasetMetric(dataset, privileged_groups=[{'gender': 1}], unprivileged_groups=[{'gender': 0}])\n",
        "print(\"Disparate Impact:\", metric.disparate_impact())\n",
        "\n",
        "# Recommendation Agents\n",
        "env = build_agent_init(env_1_args)\n",
        "random_agent = build_agent_init(random_agent_args)\n",
        "critical_agent = build_agent_init(critical_agent_args)\n",
        "\n",
        "test_agent(env, random_agent)\n",
        "test_agent(env, critical_agent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import logging\n",
        "import requests\n",
        "import optuna\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import stats\n",
        "from albumentations import Compose, RandomRotate90, Flip, Transpose, RandomBrightnessContrast\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from diffprivlib.models import LogisticRegression as DPLogisticRegression\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "# Define AdvancedNet with Residual Connections\n",
        "class AdvancedNet(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(AdvancedNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc_residual = nn.Linear(784, 128)\n",
        "        self.fc_out = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x2 = self.dropout(torch.relu(self.bn2(self.fc2(x1))))\n",
        "        x_residual = self.fc_residual(x)\n",
        "        x2 += x_residual\n",
        "        output = self.fc_out(x2)\n",
        "        return output\n",
        "\n",
        "# Data Setup\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Model, Optimizer, Criterion\n",
        "model = AdvancedNet().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# Training Loop with Mixed Precision and Gradient Accumulation\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "num_epochs = 10\n",
        "accumulation_steps = 4\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.view(-1, 784).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        labels = labels.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels) / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Save Model Checkpoints\n",
        "def save_checkpoint(model, optimizer, epoch, version=\"1.0\"):\n",
        "    checkpoint_dir = f\"checkpoints/v{version}\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "# Model Evaluation with Metrics\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.view(-1, 784).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Logging and Notifications\n",
        "def send_slack_notification(message, webhook_url):\n",
        "    payload = {\"text\": message}\n",
        "    response = requests.post(webhook_url, json=payload)\n",
        "    return response.status_code\n",
        "\n",
        "webhook_url = \"YOUR_SLACK_WEBHOOK_URL\"  # Replace with your actual Slack webhook URL\n",
        "message = \"Drift detected in the model. Immediate action required.\"\n",
        "send_slack_notification(message, webhook_url)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, filename='model_logs.log', filemode='a',\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def log_model_event(event):\n",
        "    logging.info(event)\n",
        "\n",
        "log_model_event(\"Model training started.\")\n",
        "log_model_event(\"Model training completed.\")\n",
        "\n",
        "# Alternative Recommendation System Example with Ensemble Stacking\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train base models\n",
        "rf = RandomForestClassifier()\n",
        "gb = GradientBoostingClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Stack base model predictions\n",
        "meta_features = np.column_stack((rf.predict(X_test), gb.predict(X_test)))\n",
        "\n",
        "# Train meta-model\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(meta_features, y_test)\n",
        "\n",
        "# Evaluate stacked model\n",
        "meta_predictions = meta_model.predict(meta_features)\n",
        "print(f\"Stacked model accuracy: {accuracy_score(y_test, meta_predictions)}\")\n",
        "\n",
        "# Image Augmentation Setup\n",
        "augmentor = Compose([\n",
        "    RandomRotate90(),\n",
        "    Flip(),\n",
        "    Transpose(),\n",
        "    RandomBrightnessContrast(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def augment_image(image):\n",
        "    augmented = augmentor(image=image)\n",
        "    return augmented['image']\n",
        "\n",
        "# Example usage\n",
        "# image = ...  # Load or generate your image data here\n",
        "# augmented_image = augment_image(image)\n",
        "\n",
        "# Differentially Private Logistic Regression\n",
        "dp_model = DPLogisticRegression(epsilon=1.0)\n",
        "dp_model.fit(X_train, y_train)\n",
        "dp_predictions = dp_model.predict(X_test)\n",
        "\n",
        "# Fairness Metric Calculation with AIF360\n",
        "# Assume you have a DataFrame for BinaryLabelDataset setup\n",
        "# df = ...  # Load your DataFrame here\n",
        "# dataset = BinaryLabelDataset(df=df, label_names=['label'], protected_attribute_names=['gender'])\n",
        "# metric = BinaryLabelDatasetMetric(dataset, privileged_groups=[{'gender': 1}], unprivileged_groups=[{'gender': 0}])\n",
        "# print(\"Disparate Impact:\", metric.disparate_impact())\n",
        "\n",
        "# Example of downloading MNIST dataset (already included in DataLoader setup)\n",
        "# This part is commented out as the dataset is already being downloaded in the DataLoader setup.\n",
        "# datasets.MNIST(root='./data', train=True, download=True)\n",
        "\n",
        "# Final logging\n",
        "log_model_event(\"Model evaluation completed.\")"
      ],
      "metadata": {
        "id": "vZIxgY4cul_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}