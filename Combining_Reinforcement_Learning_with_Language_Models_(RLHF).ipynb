{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7UDO+rWWm1bTDZrQdEBW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Combining_Reinforcement_Learning_with_Language_Models_(RLHF).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers trl torch"
      ],
      "metadata": {
        "id": "RDf2s-mo3GGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers trl torch --upgrade"
      ],
      "metadata": {
        "id": "_AOw1GB_Xfqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from trl import PPOTrainer, PPOConfig, create_reference_model\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        # Add pad token\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        self.tokenizer.add_tokens([\"[PAD]\"])\n",
        "        self.tokenizer.model_max_length = 50\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenizer(self.texts[idx], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=50)\n",
        "        return {key: val.squeeze(0) for key, val in item.items()}\n",
        "\n",
        "# Collate function to pad sequences to the same length\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_mask = [item['attention_mask'] for item in batch]\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    return {'input_ids': input_ids_padded, 'attention_mask': attention_mask_padded}\n",
        "\n",
        "# Example data\n",
        "texts = [\"Hello, how are you?\", \"This is an example text.\", \"Reinforcement learning with transformers.\"]\n",
        "dataset = CustomDataset(texts)\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Ensure padding token is added\n",
        "tokenizer.add_tokens([\"[PAD]\"])\n",
        "tokenizer.model_max_length = 50\n",
        "policy_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "policy_model.resize_token_embeddings(len(tokenizer))\n",
        "ref_model = create_reference_model(policy_model)\n",
        "\n",
        "# Configuration for PPO\n",
        "ppo_config = PPOConfig(\n",
        "    learning_rate=1e-5,\n",
        "    batch_size=2,\n",
        "    mini_batch_size=1,\n",
        "    output_dir=\"./ppo_output\"\n",
        ")\n",
        "\n",
        "# Mock reward model for the sake of example\n",
        "class MockRewardModel(torch.nn.Module):\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return torch.tensor([1.0])\n",
        "\n",
        "reward_model = MockRewardModel()\n",
        "\n",
        "# Initialize PPOTrainer\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    policy=policy_model,\n",
        "    ref_policy=ref_model,\n",
        "    reward_model=reward_model,\n",
        "    train_dataset=dataset,\n",
        "    value_model=policy_model,\n",
        "    processing_class=None  # Ensure you have the correct processing class as required\n",
        ")\n",
        "\n",
        "# Debugging: Ensure data is correctly passed\n",
        "print(\"Dataset length:\", len(data_loader.dataset))\n",
        "for batch in data_loader:\n",
        "    print(\"Batch input ids:\", batch['input_ids'])\n",
        "    print(\"Batch attention mask:\", batch['attention_mask'])\n",
        "    break  # Just inspect the first batch\n",
        "\n",
        "# Training loop with manual loss calculation\n",
        "optimizer = torch.optim.Adam(policy_model.parameters(), lr=ppo_config.learning_rate)\n",
        "\n",
        "for epoch in range(3):  # Simulating 3 epochs\n",
        "    for step, batch in enumerate(data_loader):\n",
        "        batch = {k: v.to(ppo_trainer.accelerator.device) for k, v in batch.items()}\n",
        "        outputs = policy_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['input_ids'])\n",
        "        loss = F.cross_entropy(outputs.logits.view(-1, outputs.logits.size(-1)), batch['input_ids'].view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item()}\")\n",
        "\n",
        "print(\"Reinforcement learning training completed.\")"
      ],
      "metadata": {
        "id": "9_u0HxxQbZfe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}