{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMsnp/oYIBj+fil5L0/I7Cs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/Hierarchical_Reinforcement_Learning_(HRL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Option policy for subtasks\n",
        "class OptionPolicy(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(OptionPolicy, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, 128)\n",
        "        self.output = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc(x))\n",
        "        return torch.softmax(self.output(x), dim=-1)  # Subtask action probabilities\n",
        "\n",
        "# High-level policy for selecting options\n",
        "class HighLevelPolicy(nn.Module):\n",
        "    def __init__(self, state_dim, num_options):\n",
        "        super(HighLevelPolicy, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, 128)\n",
        "        self.output = nn.Linear(128, num_options)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc(state))\n",
        "        return torch.softmax(self.output(x), dim=-1)  # Option probabilities\n",
        "\n",
        "# Function to select an option based on high-level policy\n",
        "def select_option(high_level_policy, state):\n",
        "    option_probs = high_level_policy(state)\n",
        "    option = torch.argmax(option_probs, dim=-1).item()  # Select the option with the highest probability\n",
        "    return option\n",
        "\n",
        "# Function to select an action based on option policy\n",
        "def select_action(option_policy, state):\n",
        "    action_probs = option_policy(state)\n",
        "    action = torch.argmax(action_probs, dim=-1).item()  # Select the action with the highest probability\n",
        "    return action\n",
        "\n",
        "# Example training function\n",
        "def train_hierarchical_policy(high_level_policy, option_policies, optimizer, episodes=1000):\n",
        "    for episode in range(episodes):\n",
        "        state = torch.randn(1, state_dim)  # Simulate a random initial state\n",
        "\n",
        "        # Select an option based on high-level policy\n",
        "        option = select_option(high_level_policy, state)\n",
        "\n",
        "        # Execute the selected option policy\n",
        "        action = select_action(option_policies[option], state)\n",
        "\n",
        "        # Simulate reward and next state (placeholder, replace with real environment interaction)\n",
        "        reward = torch.tensor(np.random.randn(), dtype=torch.float32, requires_grad=True)  # Convert to tensor with requires_grad\n",
        "        next_state = torch.randn(1, state_dim)\n",
        "\n",
        "        # Compute loss and backpropagate\n",
        "        loss = -reward  # Negative reward as loss (minimization)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f'Episode {episode}, Loss: {loss.item()}')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_dim = 10  # Example state dimension\n",
        "    action_dim = 5  # Example action dimension for each option policy\n",
        "    num_options = 3  # Number of option policies\n",
        "\n",
        "    # Initialize option policies and high-level policy\n",
        "    option_policies = [OptionPolicy(state_dim, action_dim) for _ in range(num_options)]\n",
        "    high_level_policy = HighLevelPolicy(state_dim, num_options)\n",
        "\n",
        "    # Combine parameters of high-level and option policies for the optimizer\n",
        "    params = list(high_level_policy.parameters())\n",
        "    for policy in option_policies:\n",
        "        params += list(policy.parameters())\n",
        "\n",
        "    optimizer = optim.Adam(params, lr=0.001)\n",
        "\n",
        "    # Train the hierarchical policy\n",
        "    train_hierarchical_policy(high_level_policy, option_policies, optimizer)\n",
        "\n",
        "    # Example state\n",
        "    state = torch.randn(1, state_dim)\n",
        "\n",
        "    # Select an option based on high-level policy\n",
        "    option = select_option(high_level_policy, state)\n",
        "\n",
        "    # Execute the selected option policy\n",
        "    action = select_action(option_policies[option], state)\n",
        "\n",
        "    print(f'Selected Option: {option}')\n",
        "    print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "XekhgpFCWr9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}