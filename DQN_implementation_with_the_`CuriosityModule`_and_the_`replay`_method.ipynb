{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP2OsEM07W1S+Pdry4cF3/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/DQN_implementation_with_the_%60CuriosityModule%60_and_the_%60replay%60_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGnFBr_EoGxZ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Define Curiosity Module\n",
        "class CuriosityModule(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(CuriosityModule, self).__init__()\n",
        "        self.forward_model = nn.Sequential(\n",
        "            nn.Linear(state_size + action_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, state_size)\n",
        "        )\n",
        "        self.inverse_model = nn.Sequential(\n",
        "            nn.Linear(2 * state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, next_state, action):\n",
        "        # Ensure state and next_state have the correct dimensions\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "        if next_state.dim() == 1:\n",
        "            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "        # Ensure action has two dimensions\n",
        "        if action.dim() == 1:\n",
        "            action = action.unsqueeze(0)\n",
        "\n",
        "        # Concatenate state and action\n",
        "        state_action = torch.cat([state, action], dim=1)  # Concatenate along the feature dimension\n",
        "\n",
        "        predicted_next_state = self.forward_model(state_action)\n",
        "        predicted_action = self.inverse_model(torch.cat([state, next_state], dim=1))\n",
        "        return predicted_next_state, predicted_action\n",
        "\n",
        "    def intrinsic_reward(self, next_state, predicted_next_state):\n",
        "        # Calculate the mean squared error loss for each sample (shape: [batch_size])\n",
        "        intrinsic_rewards = F.mse_loss(next_state, predicted_next_state, reduction='none')\n",
        "        intrinsic_rewards = intrinsic_rewards.mean(dim=1)  # Average MSE across the state dimension\n",
        "        return intrinsic_rewards  # Shape: [batch_size]\n",
        "\n",
        "# Define the DQN Agent with Curiosity\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.batch_size = 64\n",
        "\n",
        "        self.model = QNetwork(state_size, action_size)\n",
        "        self.target_model = QNetwork(state_size, action_size)\n",
        "        self.curiosity = CuriosityModule(state_size, action_size)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.curiosity_optimizer = optim.Adam(self.curiosity.parameters(), lr=0.001)\n",
        "\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "\n",
        "        return np.argmax(q_values.numpy())\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = torch.FloatTensor(states).view(self.batch_size, -1)  # Ensure correct shape\n",
        "        next_states = torch.FloatTensor(next_states).view(self.batch_size, -1)  # Ensure correct shape\n",
        "\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1)  # Ensure rewards is of shape [batch_size, 1]\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1)      # Ensure dones is of shape [batch_size, 1]\n",
        "\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1)   # Ensure actions is of shape [batch_size, 1]\n",
        "\n",
        "        # One-hot encode actions (this creates a 2D tensor of shape [batch_size, action_size])\n",
        "        one_hot_actions = F.one_hot(actions.squeeze(), num_classes=self.action_size).float()\n",
        "\n",
        "        # Intrinsic rewards\n",
        "        predicted_next_states, _ = self.curiosity(states, next_states, one_hot_actions)\n",
        "\n",
        "        # Adjust the shape of predicted_next_states to match next_states\n",
        "        predicted_next_states = predicted_next_states.view(next_states.size())\n",
        "\n",
        "        intrinsic_rewards = self.curiosity.intrinsic_reward(next_states, predicted_next_states)\n",
        "\n",
        "        # Add intrinsic rewards to the rewards (make sure both are 1D tensors)\n",
        "        intrinsic_rewards = intrinsic_rewards.unsqueeze(1)  # Shape: [batch_size, 1]\n",
        "        rewards = rewards + intrinsic_rewards.detach()  # Shape: [batch_size, 1]\n",
        "\n",
        "        # Compute Q targets\n",
        "        q_values = self.model(states).gather(1, actions)  # Shape: [batch_size, 1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)  # Shape: [batch_size, 1]\n",
        "            q_targets = rewards + (self.gamma * max_next_q_values * (1 - dones))\n",
        "\n",
        "        # Loss calculation and update for Q-Network\n",
        "        loss = F.mse_loss(q_values, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Curiosity module update\n",
        "        predicted_next_states, predicted_actions = self.curiosity(states, next_states, one_hot_actions)\n",
        "        curiosity_loss = F.mse_loss(predicted_next_states, next_states) + \\\n",
        "                          F.cross_entropy(predicted_actions.view(-1, self.action_size), actions.view(-1))\n",
        "\n",
        "        self.curiosity_optimizer.zero_grad()\n",
        "        curiosity_loss.backward()\n",
        "        self.curiosity_optimizer.step()\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Train the agent in a CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n  # Correct attribute for action space\n",
        "\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "episodes = 500\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            agent.update_target_model()\n",
        "            print(f\"Episode: {e+1}/{episodes}, Score: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "        agent.replay()\n",
        "\n",
        "env.close()"
      ]
    }
  ]
}