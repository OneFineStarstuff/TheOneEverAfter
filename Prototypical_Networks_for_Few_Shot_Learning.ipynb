{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPdRwl+f/Z5Z+Ze0W7+P6M6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Prototypical_Networks_for_Few_Shot_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czx8EWJ7Kdih"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Custom dataset class for few-shot learning\n",
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, dataset, indices):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[self.indices[idx]]\n",
        "        return image, label\n",
        "\n",
        "# Define the Prototypical Network\n",
        "class PrototypicalNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrototypicalNetwork, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),  # Flatten before the linear layers\n",
        "            nn.Linear(128 * 7 * 7, 256),  # Ensure dimensions match\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, support_set, query):\n",
        "        # Encode support set and query\n",
        "        support_embeddings = self.encoder(support_set)\n",
        "        query_embeddings = self.encoder(query)\n",
        "\n",
        "        return support_embeddings, query_embeddings\n",
        "\n",
        "def calculate_prototypes(support_embeddings, support_labels, num_classes=10):\n",
        "    # Create a tensor to hold the prototypes\n",
        "    prototypes = torch.zeros(num_classes, support_embeddings.size(1)).to(support_embeddings.device)\n",
        "    for i in range(num_classes):\n",
        "        class_embeddings = support_embeddings[support_labels == i]\n",
        "        if len(class_embeddings) > 0:\n",
        "            prototypes[i] = class_embeddings.mean(dim=0)\n",
        "    return prototypes\n",
        "\n",
        "def pairwise_distances(x, y):\n",
        "    n = x.size(0)\n",
        "    m = y.size(0)\n",
        "    d = x.size(1)\n",
        "    x = x.unsqueeze(1).expand(n, m, d)\n",
        "    y = y.unsqueeze(0).expand(n, m, d)\n",
        "    return torch.pow(x - y, 2).sum(2)\n",
        "\n",
        "# Training function\n",
        "def train_prototypical_network(model, support_loader, query_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    for support_set, query_set in zip(support_loader, query_loader):\n",
        "        support_images, support_labels = support_set\n",
        "        query_images, query_labels = query_set\n",
        "        support_images, query_images, support_labels, query_labels = support_images.to(device), query_images.to(device), support_labels.to(device), query_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        support_embeddings, query_embeddings = model(support_images, query_images)\n",
        "\n",
        "        # Compute prototypes\n",
        "        prototypes = calculate_prototypes(support_embeddings, support_labels)\n",
        "\n",
        "        # Compute distances\n",
        "        distances = pairwise_distances(query_embeddings, prototypes)\n",
        "\n",
        "        # Convert distances to logits\n",
        "        logits = -distances\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(logits, query_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_prototypical_network(model, support_loader, query_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for support_set, query_set in zip(support_loader, query_loader):\n",
        "            support_images, support_labels = support_set\n",
        "            query_images, query_labels = query_set\n",
        "            support_images, query_images, support_labels, query_labels = support_images.to(device), query_images.to(device), support_labels.to(device), query_labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            support_embeddings, query_embeddings = model(support_images, query_images)\n",
        "\n",
        "            # Compute prototypes\n",
        "            prototypes = calculate_prototypes(support_embeddings, support_labels)\n",
        "\n",
        "            # Compute distances\n",
        "            distances = pairwise_distances(query_embeddings, prototypes)\n",
        "\n",
        "            # Convert distances to logits\n",
        "            logits = -distances\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == query_labels).sum().item()\n",
        "            total += query_labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Load and preprocess data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "support_indices = list(range(600))  # Example support set indices\n",
        "query_indices = list(range(600, 1000))  # Example query set indices\n",
        "\n",
        "support_set = FewShotDataset(mnist_dataset, support_indices)\n",
        "query_set = FewShotDataset(mnist_dataset, query_indices)\n",
        "\n",
        "support_loader = DataLoader(support_set, batch_size=32, shuffle=True)\n",
        "query_loader = DataLoader(query_set, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define model, criterion, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PrototypicalNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_prototypical_network(model, support_loader, query_loader, criterion, optimizer, device)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluate_prototypical_network(model, support_loader, query_loader, device)\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ]
    }
  ]
}