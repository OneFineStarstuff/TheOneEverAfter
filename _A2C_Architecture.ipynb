{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwM5Z2yGQ+THJ+tu6+t2b/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/_A2C_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "import os\n",
        "\n",
        "# Define a SumTree to store priorities for sampling\n",
        "class SumTree:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.data_pointer = 0\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        tree_index = self.data_pointer + self.capacity - 1\n",
        "        self.data[self.data_pointer] = data\n",
        "        self.update(tree_index, priority)\n",
        "        self.data_pointer += 1\n",
        "        if self.data_pointer >= self.capacity:\n",
        "            self.data_pointer = 0\n",
        "\n",
        "    def update(self, tree_index, priority):\n",
        "        change = priority - self.tree[tree_index]\n",
        "        self.tree[tree_index] = priority\n",
        "        self._propagate(tree_index, change)\n",
        "\n",
        "    def _propagate(self, tree_index, change):\n",
        "        parent = (tree_index - 1) // 2\n",
        "        self.tree[parent] += change\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def get_leaf(self, value):\n",
        "        parent = 0\n",
        "        while True:\n",
        "            left = 2 * parent + 1\n",
        "            right = left + 1\n",
        "            if left >= len(self.tree):\n",
        "                leaf = parent\n",
        "                break\n",
        "            else:\n",
        "                if value <= self.tree[left]:\n",
        "                    parent = left\n",
        "                else:\n",
        "                    value -= self.tree[left]\n",
        "                    parent = right\n",
        "        data_index = leaf - self.capacity + 1\n",
        "        return leaf, self.tree[leaf], self.data[data_index]\n",
        "\n",
        "    @property\n",
        "    def total_priority(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "# Define Prioritized Replay Buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.max_priority = 1.0\n",
        "\n",
        "    def add(self, experience):\n",
        "        priority = self.max_priority ** self.alpha\n",
        "        self.tree.add(priority, experience)\n",
        "\n",
        "    def sample(self, batch_size, beta):\n",
        "        experiences = []\n",
        "        indices = []\n",
        "        priorities = []\n",
        "        segment = self.tree.total_priority / batch_size\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            value = random.uniform(a, b)\n",
        "            index, priority, experience = self.tree.get_leaf(value)\n",
        "            experiences.append(experience)\n",
        "            indices.append(index)\n",
        "            priorities.append(priority)\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total_priority\n",
        "        is_weight = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
        "        is_weight /= is_weight.max()\n",
        "\n",
        "        return experiences, indices, is_weight\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for index, priority in zip(indices, priorities):\n",
        "            self.tree.update(index, priority)\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "\n",
        "# Define the NoisyLinear layer\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features).fill_(sigma_init))\n",
        "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features).fill_(sigma_init))\n",
        "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / np.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_epsilon = torch.normal(mean=0.0, std=1.0, size=self.weight_epsilon.shape, device=self.weight_epsilon.device)\n",
        "        bias_epsilon = torch.normal(mean=0.0, std=1.0, size=self.bias_epsilon.shape, device=self.bias_epsilon.device)\n",
        "        weight = self.weight_mu + self.weight_sigma * weight_epsilon\n",
        "        bias = self.bias_mu + self.bias_sigma * bias_epsilon\n",
        "        return torch.nn.functional.linear(x, weight, bias)\n",
        "\n",
        "# Define the Dueling Neural Network with Noisy Linear layers\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.fc1 = NoisyLinear(state_dim, 128)\n",
        "\n",
        "        # Value stream\n",
        "        self.value_fc = NoisyLinear(128, 64)\n",
        "        self.value = NoisyLinear(64, 1)\n",
        "\n",
        "        # Advantage stream\n",
        "        self.advantage_fc = NoisyLinear(128, 64)\n",
        "        self.advantage = NoisyLinear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Calculate value and advantage\n",
        "        value = torch.relu(self.value_fc(x))\n",
        "        value = self.value(value)\n",
        "\n",
        "        advantage = torch.relu(self.advantage_fc(x))\n",
        "        advantage = self.advantage(advantage)\n",
        "\n",
        "        # Combine value and advantage\n",
        "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_values\n",
        "\n",
        "# Define the Policy Network for Policy Gradient\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return torch.softmax(self.fc2(x), dim=-1)  # Action probabilities\n",
        "\n",
        "# Define the Actor-Critic Network\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "\n",
        "        # Actor (Policy)\n",
        "        self.actor = nn.Linear(128, action_dim)\n",
        "\n",
        "        # Critic (Value)\n",
        "        self.critic = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        policy = torch.softmax(self.actor(x), dim=-1)  # Action probabilities\n",
        "        value = self.critic(x)  # State value\n",
        "        return policy, value\n",
        "\n",
        "# Hyperparameters\n",
        "GAMMA = 0.99            # Discount factor for future rewards\n",
        "LR = 1e-3               # Learning rate\n",
        "BATCH_SIZE = 64         # Batch size for experience replay\n",
        "EPSILON_START = 1.0     # Initial epsilon for exploration\n",
        "EPSILON_END = 0.01      # Minimum epsilon\n",
        "EPSILON_DECAY = 0.995   # Decay rate for epsilon\n",
        "TARGET_UPDATE = 10      # Update target network every 10 episodes\n",
        "ALPHA = 0.6             # Prioritization exponent\n",
        "BETA_START = 0.4        # Initial beta value for importance sampling\n",
        "BETA_INCREMENT = 1e-3   # Beta increment per episode\n",
        "CHECKPOINT_DIR = './checkpoints' # Directory to save checkpoints\n",
        "\n",
        "# Create checkpoint directory if it does not exist\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\", new_step_api=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Initialize networks\n",
        "policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "value_net = DuelingDQN(state_dim, action_dim)\n",
        "target_net = DuelingDQN(state_dim, action_dim)\n",
        "actor_critic_net = ActorCriticNetwork(state_dim, action_dim)\n",
        "target_net.load_state_dict(value_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "# Initialize optimizers\n",
        "optimizer_policy = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "optimizer_value = optim.Adam(value_net.parameters(), lr=LR)\n",
        "optimizer_actor_critic = optim.Adam(actor_critic_net.parameters(), lr=LR)\n",
        "\n",
        "# Initialize prioritized replay buffer\n",
        "memory = PrioritizedReplayBuffer(capacity=10000, alpha=ALPHA)\n",
        "\n",
        "# Function to compute returns\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for reward in reversed(rewards):\n",
        "        G = reward + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    return torch.tensor(returns)\n",
        "\n",
        "# Function to train policy gradient\n",
        "def train_policy_gradient(episode_states, episode_actions, episode_rewards):\n",
        "    returns = compute_returns(episode_rewards)\n",
        "\n",
        "    # Normalize returns for stable training\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "    policy_loss = []\n",
        "    for state, action, G in zip(episode_states, episode_actions, returns):\n",
        "        state = torch.FloatTensor(state)\n",
        "        action = torch.tensor(action)\n",
        "\n",
        "        # Compute log-probability and loss\n",
        "        action_prob = policy_net(state)\n",
        "        log_prob = torch.log(action_prob[action])\n",
        "        policy_loss.append(-log_prob * G)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer_policy.zero_grad()\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer_policy.step()\n",
        "\n",
        "# Function to train actor-critic network\n",
        "def train_actor_critic(states, actions, rewards, next_states, dones, gamma=0.99):\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    next_states = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Forward pass\n",
        "    policy, value = actor_critic_net(states)\n",
        "    _, next_value = actor_critic_net(next_states)\n",
        "\n",
        "    # Compute advantage\n",
        "    target_value = rewards + gamma * next_value * (1 - dones)\n",
        "    advantage = target_value - value\n",
        "\n",
        "    # Compute policy loss\n",
        "    log_probs = torch.log(policy.gather(1, actions.unsqueeze(1)).squeeze())\n",
        "    policy_loss = -(log_probs * advantage.detach()).mean()\n",
        "\n",
        "    # Compute value loss\n",
        "    value_loss = advantage.pow(2).mean()\n",
        "\n",
        "    # Backpropagation\n",
        "    loss = policy_loss + value_loss\n",
        "    optimizer_actor_critic.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_actor_critic.step()\n",
        "\n",
        "# Function to select an action\n",
        "def select_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    with torch.no_grad():\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = value_net(state)\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "# Function to store experiences in memory\n",
        "def store_experience(state, action, reward, next_state, done):\n",
        "    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "# Function to sample and train the model with Double DQN\n",
        "def optimize_model_double_dqn(beta):\n",
        "    if len(memory.tree.data) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # Sample a batch of experiences from memory\n",
        "    experiences, indices, is_weight = memory.sample(BATCH_SIZE, beta)\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "    states = torch.FloatTensor(np.array(states))\n",
        "    actions = torch.LongTensor(np.array(actions)).unsqueeze(1)\n",
        "    rewards = torch.FloatTensor(np.array(rewards))\n",
        "    next_states = torch.FloatTensor(np.array(next_states))\n",
        "    dones = torch.FloatTensor(np.array(dones))\n",
        "    is_weight = torch.FloatTensor(is_weight)\n",
        "\n",
        "    # Get Q values for current states\n",
        "    current_q_values = value_net(states).gather(1, actions).squeeze()\n",
        "\n",
        "    # Double DQN update: use policy network for action selection and target network for Q-value calculation\n",
        "    next_actions = value_net(next_states).argmax(1).unsqueeze(1)\n",
        "    next_q_values = target_net(next_states).gather(1, next_actions).squeeze()\n",
        "    target_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
        "\n",
        "    # Compute loss and optimize\n",
        "    loss = (current_q_values - target_q_values.detach()).pow(2) * is_weight\n",
        "    loss = loss.mean()\n",
        "    optimizer_value.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_value.step()\n",
        "\n",
        "    # Update priorities in memory\n",
        "    new_priorities = (current_q_values - target_q_values.detach()).abs().detach().cpu().numpy()\n",
        "    memory.update_priorities(indices, new_priorities + 1e-5)\n",
        "\n",
        "# Load checkpoint if available\n",
        "def load_checkpoint(path, value_net, optimizer):\n",
        "    if os.path.isfile(path):\n",
        "        checkpoint = torch.load(path, weights_only=True)\n",
        "        value_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        return checkpoint['episode']\n",
        "    return 0\n",
        "\n",
        "# Save model checkpoints\n",
        "def save_checkpoint(episode, value_net, optimizer, path):\n",
        "    torch.save({\n",
        "        'episode': episode,\n",
        "        'model_state_dict': value_net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n",
        "\n",
        "# Enable anomaly detection\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 500\n",
        "epsilon = EPSILON_START\n",
        "beta = BETA_START\n",
        "\n",
        "start_episode = load_checkpoint(os.path.join(CHECKPOINT_DIR, 'dqn_checkpoint.pth'), value_net, optimizer_value)\n",
        "\n",
        "for episode in range(start_episode, num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    episode_next_states = []\n",
        "    episode_dones = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(200):\n",
        "        action = select_action(state, epsilon)\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "        episode_next_states.append(next_state)\n",
        "        episode_dones.append(done or truncated)\n",
        "\n",
        "        store_experience(state, action, reward, next_state, done or truncated)\n",
        "        state = next_state\n",
        "\n",
        "        optimize_model_double_dqn(beta)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    train_policy_gradient(episode_states, episode_actions, episode_rewards)\n",
        "\n",
        "    train_actor_critic(episode_states, episode_actions, episode_rewards, episode_next_states, episode_dones)\n",
        "\n",
        "    # Decay epsilon for exploration-exploitation trade-off\n",
        "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
        "    beta = min(1.0, beta + BETA_INCREMENT)\n",
        "\n",
        "    # Update target network every TARGET_UPDATE episodes\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(value_net.state_dict())\n",
        "\n",
        "    # Save checkpoint\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        save_checkpoint(episode, value_net, optimizer_value, os.path.join(CHECKPOINT_DIR, 'dqn_checkpoint.pth'))\n",
        "\n",
        "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "RK_KAm64XAna"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}