{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOhUq1WMGNy/sVxUu+uOhFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/Deep_Deterministic_Policy_Gradient_(DDPG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Actor network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x)) * self.max_action\n",
        "        return x\n",
        "\n",
        "# Critic network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1000000):\n",
        "        self.storage = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, transition):\n",
        "        self.storage.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.storage, batch_size)\n",
        "        state, action, next_state, reward, done = zip(*batch)\n",
        "        return np.array(state), np.array(action), np.array(next_state), np.array(reward), np.array(done)\n",
        "\n",
        "# DDPG agent\n",
        "class DDPG:\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        self.max_action = max_action\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "\n",
        "    def select_action(self, state, noise=0.1):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "        # Add noise for exploration\n",
        "        action += np.random.normal(0, noise, size=action.shape)\n",
        "\n",
        "        return np.clip(action, -self.max_action, self.max_action)\n",
        "\n",
        "    def train(self, batch_size=100):\n",
        "        if len(self.replay_buffer.storage) < batch_size:\n",
        "            return\n",
        "\n",
        "        state, action, next_state, reward, done = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        action = torch.FloatTensor(action).to(device)\n",
        "        next_state = torch.FloatTensor(next_state).to(device)\n",
        "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
        "        done = torch.FloatTensor(done).unsqueeze(1).to(device)\n",
        "\n",
        "        # Critic loss\n",
        "        target_q = self.critic_target(next_state, self.actor_target(next_state))\n",
        "        target_q = reward + ((1 - done) * self.gamma * target_q).detach()\n",
        "\n",
        "        current_q = self.critic(state, action)\n",
        "\n",
        "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor loss\n",
        "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Soft update\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "    def add_to_replay_buffer(self, transition):\n",
        "        self.replay_buffer.add(transition)\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.actor.state_dict(), filename + \"_actor.pth\")\n",
        "        torch.save(self.critic.state_dict(), filename + \"_critic.pth\")\n",
        "\n",
        "    def load(self, filename):\n",
        "        self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
        "        self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n",
        "\n",
        "# Training DDPG\n",
        "def train_ddpg(env_name, max_episodes=500, max_steps=1000):\n",
        "    env = gym.make(env_name)  # Ensure the new step API is used if necessary\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = env.action_space.high[0]\n",
        "\n",
        "    agent = DDPG(state_dim, action_dim, max_action)\n",
        "    running_reward = 0\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            agent.add_to_replay_buffer((state, action, next_state, reward, float(done)))\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            agent.train()\n",
        "\n",
        "            running_reward += episode_reward\n",
        "\n",
        "            print(f'Episode {episode}, Step {t}, Reward: {episode_reward}, Average Reward: {running_reward / (episode + 1):.2f}')\n",
        "\n",
        "    agent.save(\"ddpg_model\")  # Save model after training\n",
        "    env.close()\n",
        "\n",
        "# Define your device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train DDPG on Pendulum-v1\n",
        "train_ddpg(\"Pendulum-v1\")"
      ],
      "metadata": {
        "id": "9MBqw-VKvZjk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}