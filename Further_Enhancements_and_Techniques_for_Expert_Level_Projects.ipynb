{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZhrmm3oKc9S42LE23L+jp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Further_Enhancements_and_Techniques_for_Expert_Level_Projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "id": "Y6OCf77QCSNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data to fit model input shape (28, 28, 1)\n",
        "x_train_cnn = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test_cnn = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Create a validation set from the training data\n",
        "x_train_cnn, x_val_cnn = x_train_cnn[:-12000], x_train_cnn[-12000:]\n",
        "y_train, y_val = y_train[:-12000], y_train[-12000:]\n",
        "\n",
        "# Create a sequential model\n",
        "model_cnn = models.Sequential()\n",
        "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "model_cnn.add(layers.Flatten())\n",
        "model_cnn.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))) # Regularization\n",
        "model_cnn.add(layers.Dropout(0.5))  # Dropout regularization\n",
        "model_cnn.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "datagen.fit(x_train_cnn)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "# Train the model with augmented data and learning rate scheduler\n",
        "model_cnn.fit(datagen.flow(x_train_cnn, y_train, batch_size=32), epochs=5, validation_data=(x_val_cnn, y_val), callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(x_test_cnn, y_test)\n",
        "print(f'Test accuracy: {test_acc_cnn}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_cnn = model_cnn.predict(x_test_cnn)\n",
        "\n",
        "# Plotting some test images with their predicted labels\n",
        "for i in range(5):\n",
        "    plt.imshow(x_test_cnn[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f'Predicted label: {np.argmax(predictions_cnn[i])}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Resize MNIST images to 224x224 to match VGG16 input shape\n",
        "def resize_images(images):\n",
        "    resized_images = np.zeros((images.shape[0], 224, 224, 3))\n",
        "    for i in range(images.shape[0]):\n",
        "        img = array_to_img(images[i].reshape(28, 28, 1))\n",
        "        img = img.resize((224, 224))\n",
        "        img = img.convert(\"RGB\") # Convert grayscale to RGB\n",
        "        resized_images[i] = img_to_array(img)\n",
        "    return resized_images / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "x_train_vgg = resize_images(x_train_cnn)\n",
        "x_test_vgg = resize_images(x_test_cnn)\n",
        "x_val_vgg = resize_images(x_val_cnn)\n",
        "\n",
        "# Transfer Learning with VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the base model\n",
        "model_transfer = models.Sequential()\n",
        "model_transfer.add(base_model)\n",
        "model_transfer.add(layers.Flatten())\n",
        "model_transfer.add(layers.Dense(256, activation='relu'))\n",
        "model_transfer.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile and train the new model\n",
        "model_transfer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_transfer.fit(x_train_vgg, y_train, epochs=5, validation_data=(x_val_vgg, y_val))\n",
        "\n",
        "# Hyperparameter Tuning with Keras Tuner\n",
        "def build_model(hp):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(hp.Int('conv_units', min_value=32, max_value=128, step=32), (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(hp.Int('dense_units', min_value=64, max_value=256, step=64), activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=5)\n",
        "\n",
        "tuner.search(x_train_cnn, y_train, epochs=5, validation_data=(x_val_cnn, y_val))"
      ],
      "metadata": {
        "id": "rU6Lw61KAvyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyql7e08-L5I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data to fit model input shape (28, 28, 1)\n",
        "x_train_cnn = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test_cnn = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Create a validation set from the training data\n",
        "x_train_cnn, x_val_cnn = x_train_cnn[:-12000], x_train_cnn[-12000:]\n",
        "y_train, y_val = y_train[:-12000], y_train[-12000:]\n",
        "\n",
        "# Create a sequential model\n",
        "model_cnn = models.Sequential()\n",
        "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "model_cnn.add(layers.Flatten())\n",
        "model_cnn.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))) # Regularization\n",
        "model_cnn.add(layers.Dropout(0.5))  # Dropout regularization\n",
        "model_cnn.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "datagen.fit(x_train_cnn)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "# Train the model with augmented data and learning rate scheduler\n",
        "model_cnn.fit(datagen.flow(x_train_cnn, y_train, batch_size=32), epochs=5, validation_data=(x_val_cnn, y_val), callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(x_test_cnn, y_test)\n",
        "print(f'Test accuracy: {test_acc_cnn}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_cnn = model_cnn.predict(x_test_cnn)\n",
        "\n",
        "# Plotting some test images with their predicted labels\n",
        "for i in range(5):\n",
        "    plt.imshow(x_test_cnn[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f'Predicted label: {np.argmax(predictions_cnn[i])}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data to fit model input shape (28, 28, 1)\n",
        "x_train_cnn = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test_cnn = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Create an instance of ImageDataGenerator with augmentation options\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "datagen.fit(x_train_cnn)\n",
        "\n",
        "# Create a validation set from the training data\n",
        "x_train_cnn, x_val_cnn = x_train_cnn[:-12000], x_train_cnn[-12000:]\n",
        "y_train, y_val = y_train[:-12000], y_train[-12000:]\n",
        "\n",
        "# Load a pre-trained VGG16 model without the top layer (the classification part)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model on top of the base model\n",
        "model_transfer = models.Sequential()\n",
        "model_transfer.add(layers.UpSampling2D(size=(8, 8), input_shape=(28, 28, 1)))  # Resizing input to match VGG16\n",
        "model_transfer.add(layers.Conv2D(3, (3, 3), padding='same'))  # Matching channel number to 3\n",
        "model_transfer.add(base_model)\n",
        "model_transfer.add(layers.Flatten())\n",
        "model_transfer.add(layers.Dense(256, activation='relu'))\n",
        "model_transfer.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile and train the new model\n",
        "model_transfer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_transfer.fit(x_train_cnn, y_train, epochs=5, validation_data=(x_val_cnn, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_transfer, test_acc_transfer = model_transfer.evaluate(x_test_cnn, y_test)\n",
        "print(f'Test accuracy: {test_acc_transfer}')"
      ],
      "metadata": {
        "id": "WS4m66NL_wFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras_tuner import RandomSearch\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data to fit model input shape (28, 28, 1)\n",
        "x_train_cnn = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test_cnn = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Define the model building function\n",
        "def build_model(hp):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(hp.Int('conv_units', min_value=32, max_value=128, step=32), (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(hp.Int('dense_units', min_value=64, max_value=256, step=64), activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = RandomSearch(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=5)\n",
        "\n",
        "# Perform the search\n",
        "tuner.search(x_train_cnn, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f'Best hyperparameters: Conv units: {best_hps.get(\"conv_units\")}, Dense units: {best_hps.get(\"dense_units\")}')"
      ],
      "metadata": {
        "id": "jKvenO0x_zS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}