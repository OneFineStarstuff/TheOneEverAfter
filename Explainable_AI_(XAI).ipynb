{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNvtcEFSqeOD8KekHoTaufz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/Explainable_AI_(XAI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras_tuner"
      ],
      "metadata": {
        "id": "3LO6TDy3xHpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7HRxbUhwPeI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to values between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Reshape data to fit model input shape (28, 28, 1)\n",
        "x_train_cnn = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test_cnn = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# Create a validation set from the training data\n",
        "x_train_cnn, x_val_cnn = x_train_cnn[:-12000], x_train_cnn[-12000:]\n",
        "y_train, y_val = y_train[:-12000], y_train[-12000:]\n",
        "\n",
        "# Create a sequential model\n",
        "model_cnn = models.Sequential()\n",
        "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "model_cnn.add(layers.Flatten())\n",
        "model_cnn.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))) # Regularization\n",
        "model_cnn.add(layers.Dropout(0.5))  # Dropout regularization\n",
        "model_cnn.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "datagen.fit(x_train_cnn)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "# Train the model with augmented data and learning rate scheduler\n",
        "model_cnn.fit(datagen.flow(x_train_cnn, y_train, batch_size=32), epochs=5, validation_data=(x_val_cnn, y_val), callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(x_test_cnn, y_test)\n",
        "print(f'Test accuracy: {test_acc_cnn}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_cnn = model_cnn.predict(x_test_cnn)\n",
        "\n",
        "# Plotting some test images with their predicted labels\n",
        "for i in range(5):\n",
        "    plt.imshow(x_test_cnn[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(f'Predicted label: {np.argmax(predictions_cnn[i])}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# SHAP for model interpretability\n",
        "import shap\n",
        "\n",
        "# Use a smaller sample of your data to save time and memory\n",
        "sample_indices = np.random.choice(x_train_cnn.shape[0], 1000, replace=False)\n",
        "sample_data = x_train_cnn[sample_indices]\n",
        "\n",
        "# Initialize DeepExplainer\n",
        "explainer = shap.DeepExplainer(model_cnn, sample_data)\n",
        "\n",
        "# Explain predictions\n",
        "shap_values = explainer.shap_values(x_test_cnn[:100])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.summary_plot(shap_values, x_test_cnn[:100])"
      ]
    }
  ]
}