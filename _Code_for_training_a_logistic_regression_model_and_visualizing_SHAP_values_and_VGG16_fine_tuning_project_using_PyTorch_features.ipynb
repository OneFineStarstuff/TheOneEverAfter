{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPDfR6mR+LAbKdAN4ddG/y6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Code_for_training_a_logistic_regression_model_and_visualizing_SHAP_values_and_VGG16_fine_tuning_project_using_PyTorch_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "yA-Pen36IUgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap torch scikit-learn numpy"
      ],
      "metadata": {
        "id": "dxPjKJZMGHjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade shap"
      ],
      "metadata": {
        "id": "iEoeBzDcKni3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create synthetic data\n",
        "X, y = make_classification(n_samples=500, n_features=20, n_classes=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Initialize SHAP explainer\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "\n",
        "# Generate SHAP values for the test set\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Check the number of samples and features for SHAP values\n",
        "num_samples = X_test.shape[0]\n",
        "num_features = X_test.shape[1]\n",
        "\n",
        "# Verify SHAP values shape\n",
        "print(f\"SHAP values shape: {shap_values.shape}\")\n",
        "\n",
        "# Ensure the SHAP values' shape matches the input data\n",
        "if shap_values.values.shape == (num_samples, num_features):\n",
        "    print(\"SHAP values match the input data shape.\")\n",
        "\n",
        "    # Use SHAP values for visualization\n",
        "    shap.summary_plot(shap_values, X_test, feature_names=[f\"Feature {i}\" for i in range(X_test.shape[1])])\n",
        "\n",
        "    # Feature names\n",
        "    feature_names = [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "    # Force plot for the first sample\n",
        "    shap.force_plot(shap_values[0].base_values, shap_values.values[0], X_test[0], feature_names=feature_names, matplotlib=True)\n",
        "else:\n",
        "    print(\"The SHAP values' shape does not match the input data.\")"
      ],
      "metadata": {
        "id": "irjc4JJuquaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'model' is your trained model and 'X_test' is your test data\n",
        "\n",
        "# Create a SHAP explainer for the model\n",
        "explainer = shap.Explainer(model, X_test)\n",
        "\n",
        "# Calculate SHAP values for all samples\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Check the shape of SHAP values and X_test\n",
        "print(\"SHAP values shape:\", shap_values.shape)  # Expected (100, 20)\n",
        "print(\"X_test shape:\", X_test.shape)  # Expected (100, 20)\n",
        "\n",
        "# Extract SHAP values for class 0 for each sample (all features)\n",
        "# Now we capture all feature SHAP values for class 0 for each sample\n",
        "shap_values_class_0 = np.array([shap_values[i].values for i in range(len(shap_values))])\n",
        "\n",
        "# Check the shape of the extracted SHAP values for class 0\n",
        "print(f\"SHAP values for class 0 shape (after collection): {shap_values_class_0.shape}\")\n",
        "\n",
        "# Ensure the SHAP values for class 0 have the shape (100, 20)\n",
        "# Since each sample should have SHAP values for all features (20 features for 100 samples)\n",
        "assert shap_values_class_0.shape == (X_test.shape[0], X_test.shape[1]), \\\n",
        "    f\"Expected shape (100, 20), but got {shap_values_class_0.shape}\"\n",
        "\n",
        "# Create feature names (optional: customize if you have actual feature names)\n",
        "feature_names = [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
        "\n",
        "# Generate the SHAP summary plot for class 0\n",
        "shap.summary_plot(shap_values_class_0, X_test, feature_names=feature_names)\n",
        "\n",
        "# Optionally, generate the bar plot for feature importance\n",
        "shap.summary_plot(shap_values_class_0, X_test, plot_type=\"bar\", feature_names=feature_names)\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IZkhhmUSK4tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "# Ensure that the number of informative, redundant, and repeated features is less than total features\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 8)  # Input layer with 5 features\n",
        "        self.fc2 = nn.Linear(8, 4)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(4, 2)  # Output layer (2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # ReLU activation function\n",
        "        x = self.fc3(x)  # Output layer (no activation function here)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "3RiOlP-pe8H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "# Ensure that the number of informative, redundant, and repeated features is less than total features\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 8)  # Input layer with 5 features\n",
        "        self.fc2 = nn.Linear(8, 4)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(4, 2)  # Output layer (2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # ReLU activation function\n",
        "        x = self.fc3(x)  # Output layer (no activation function here)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "FoqkNpXthMkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "# Ensuring number of informative features is less than the total number of features\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 8)  # Input layer with 5 features\n",
        "        self.fc2 = nn.Linear(8, 4)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(4, 2)  # Output layer (2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # ReLU activation function\n",
        "        x = self.fc3(x)  # Output layer (no activation function here)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "DsMFchaLhoXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "# Ensuring number of informative features is less than the total number of features\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 8)  # Input layer with 5 features\n",
        "        self.fc2 = nn.Linear(8, 4)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(4, 2)  # Output layer (2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # ReLU activation function\n",
        "        x = self.fc3(x)  # Output layer (no activation function here)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "rnwLd6QniKwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 8)  # Input layer with 5 features\n",
        "        self.fc2 = nn.Linear(8, 4)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(4, 2)  # Output layer (2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # ReLU activation function\n",
        "        x = self.fc3(x)  # Output layer (no activation function here)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "FOt_lXH-ih7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 8)  # Input layer with 5 features\n",
        "        self.fc2 = nn.Linear(8, 4)  # Hidden layer\n",
        "        self.fc3 = nn.Linear(4, 2)  # Output layer (2 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation function\n",
        "        x = torch.relu(self.fc2(x))  # ReLU activation function\n",
        "        x = self.fc3(x)  # Output layer (no activation function here)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "ZGCitdX6i4J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 16)  # Input layer with 5 features, 16 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(16)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(16, 8)  # Hidden layer with 8 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(8)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(8, 2)   # Output layer (2 classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer to prevent overfitting\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU + BatchNorm + FC1\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU + BatchNorm + FC2\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Increased epochs for better convergence\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "wrXQCvfmjPSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # Increased neurons in the first layer\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Hidden layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer (2 classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer to prevent overfitting\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU + BatchNorm + FC1\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU + BatchNorm + FC2\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "c2IFnNSTjlSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # Increased neurons in the first layer\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Hidden layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer (2 classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer to prevent overfitting\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU + BatchNorm + FC1\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU + BatchNorm + FC2\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis\n",
        "# Wrap the model's forward function to accept NumPy arrays for SHAP compatibility\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return the raw output (logits)\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Use a small subset for background data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values for the first sample\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "pJSBVAiBj8sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "qQd5zBGskSu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "wjWq5FwXkpRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "Y-WvrUE6k9mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "0_zFhDKxlR_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "_tzywTYUlnxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "GbQZByjxl8QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "LlhPtI3amQFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "VvR4ZszOmjI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "R5fVbBAtm1gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "xvJAK3zunKoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "1SCCLEmgndb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "l6XZ6pcjnx6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "CdziE9bMoHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with Batch Normalization and Dropout\n",
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 32)  # First layer with 32 neurons\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # Batch Normalization after first layer\n",
        "        self.fc2 = nn.Linear(32, 16)  # Second layer with 16 neurons\n",
        "        self.bn2 = nn.BatchNorm1d(16)  # Batch Normalization after second layer\n",
        "        self.fc3 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer with 50% drop rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # ReLU activation + BatchNorm + First layer\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # ReLU activation + BatchNorm + Second layer\n",
        "        x = self.dropout(x)  # Dropout applied after hidden layers\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = EnhancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropy for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# 7. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 8. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 9. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "HePww1hsocCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0SW2LOLpYDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with advanced features\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 64)  # First layer with 64 neurons\n",
        "        self.fc2 = nn.Linear(64, 32)  # Second layer with 32 neurons\n",
        "        self.fc3 = nn.Linear(32, 16)  # Third layer with 16 neurons\n",
        "        self.fc4 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # Batch Normalization after first hidden layer\n",
        "        self.bn2 = nn.BatchNorm1d(32)  # Batch Normalization after second hidden layer\n",
        "        self.bn3 = nn.BatchNorm1d(16)  # Batch Normalization after third hidden layer\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% rate\n",
        "\n",
        "        # Weight Initialization (He Initialization for better convergence)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # First layer with ReLU activation and BatchNorm\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # Second layer with ReLU activation and BatchNorm\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))  # Third layer with ReLU activation and BatchNorm\n",
        "        x = self.dropout(x)  # Apply dropout to prevent overfitting\n",
        "        x = self.fc4(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For binary classification, use CrossEntropyLoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "# 7. Learning Rate Scheduler (ReduceLROnPlateau)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "# 8. Train the model\n",
        "num_epochs = 20  # Training for 20 epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Adjust learning rate based on validation loss (scheduler)\n",
        "    scheduler.step(loss)\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:  # Print loss every 2 epochs\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 9. Evaluate the model\n",
        "model.eval()  # Switch model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "    correct = (predicted == y_test_tensor).sum().item()  # Compare predictions to actual labels\n",
        "    accuracy = correct / y_test_tensor.size(0) * 100  # Calculate accuracy\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 10. SHAP analysis for model interpretability\n",
        "# Define a function to wrap the model for SHAP analysis (works with NumPy input)\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])  # Using a small subset of training data\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "JuTq9pNAo0b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# 1. Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Split the dataset into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 3. Normalize the dataset using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. Define the neural network model with advanced features\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 64)  # First layer with 64 neurons\n",
        "        self.fc2 = nn.Linear(64, 32)  # Second layer with 32 neurons\n",
        "        self.fc3 = nn.Linear(32, 16)  # Third layer with 16 neurons\n",
        "        self.fc4 = nn.Linear(16, 2)   # Output layer with 2 neurons (for binary classification)\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # Batch Normalization after first hidden layer\n",
        "        self.bn2 = nn.BatchNorm1d(32)  # Batch Normalization after second hidden layer\n",
        "        self.bn3 = nn.BatchNorm1d(16)  # Batch Normalization after third hidden layer\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% rate\n",
        "\n",
        "        # Weight Initialization (He Initialization for better convergence)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # First layer with ReLU activation and BatchNorm\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))  # Second layer with ReLU activation and BatchNorm\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))  # Third layer with ReLU activation and BatchNorm\n",
        "        x = self.dropout(x)  # Apply dropout to prevent overfitting\n",
        "        x = self.fc4(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# 6. Initialize the model, loss function, and optimizer\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss()  # For binary classification, use CrossEntropyLoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "# 7. Learning Rate Scheduler (ReduceLROnPlateau)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "# 8. Training and validation loop with model saving based on validation accuracy\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step(loss)  # Scheduler based on training loss\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / y_val_tensor.size(0) * 100\n",
        "\n",
        "    # Save best model\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Print progress every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 9. Evaluate the model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, test_predicted = torch.max(test_outputs, 1)\n",
        "    test_correct = (test_predicted == y_test_tensor).sum().item()\n",
        "    test_accuracy = test_correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 10. SHAP analysis for model interpretability\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()  # Return raw logits for SHAP\n",
        "\n",
        "# Initialize SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])\n",
        "\n",
        "# Calculate SHAP values for the first 10 test samples\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "5XKiWy_GxX7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# 1. Dataset generation\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 2. Advanced model\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.bn3 = nn.BatchNorm1d(16)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# 3. Initialization and configurations\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.fc1.parameters(), 'lr': 0.001},\n",
        "    {'params': model.fc2.parameters(), 'lr': 0.0005},\n",
        "    {'params': model.fc3.parameters(), 'lr': 0.0001},\n",
        "    {'params': model.fc4.parameters(), 'lr': 0.0001}\n",
        "])\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "clip_value = 1.0  # For gradient clipping\n",
        "early_stopping_patience = 5  # Early stopping if no improvement in 5 epochs\n",
        "\n",
        "# 4. Training loop with early stopping and gradient clipping\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model = None\n",
        "no_improvement_epochs = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "    optimizer.step()\n",
        "    scheduler.step(loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / y_val_tensor.size(0) * 100\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        no_improvement_epochs = 0  # Reset counter if improved\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    # Early stopping condition\n",
        "    if no_improvement_epochs >= early_stopping_patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Load best model based on validation accuracy\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 5. Test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, test_predicted = torch.max(test_outputs, 1)\n",
        "    test_correct = (test_predicted == y_test_tensor).sum().item()\n",
        "    test_accuracy = test_correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 6. SHAP Analysis\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()\n",
        "\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Plot SHAP summary\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "piNX8N-Pp7sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# 1. Dataset generation\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 2. Advanced model with higher dropout and weight decay\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.bn3 = nn.BatchNorm1d(16)\n",
        "        self.dropout = nn.Dropout(p=0.6)  # Increased dropout\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# 3. Initialization and configurations\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Added weight decay\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "clip_value = 1.0\n",
        "early_stopping_patience = 5\n",
        "\n",
        "# 4. Training loop with gradient clipping and cosine annealing scheduler\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model = None\n",
        "no_improvement_epochs = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / y_val_tensor.size(0) * 100\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        no_improvement_epochs = 0\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    # Early stopping condition\n",
        "    if no_improvement_epochs >= early_stopping_patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Load best model based on validation accuracy\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 5. Test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, test_predicted = torch.max(test_outputs, 1)\n",
        "    test_correct = (test_predicted == y_test_tensor).sum().item()\n",
        "    test_accuracy = test_correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 6. SHAP Analysis\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()\n",
        "\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Plot SHAP summary\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "Bj0cyEmLqYTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# 1. Dataset generation\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 2. Optimized model with layer normalization and warm-up\n",
        "class OptimizedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OptimizedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 12)\n",
        "        self.fc4 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.ln3(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# 3. Initialization and configurations\n",
        "model = OptimizedNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
        "clip_value = 1.0\n",
        "early_stopping_patience = 5\n",
        "\n",
        "# Warm-up scheduler for the first few epochs\n",
        "warmup_epochs = 5\n",
        "warmup_scheduler = lambda epoch: (epoch + 1) / warmup_epochs if epoch < warmup_epochs else 1.0\n",
        "\n",
        "# 4. Training loop with gradient clipping, warm-up, and scheduler\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model = None\n",
        "no_improvement_epochs = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.param_groups[0]['lr'] = warmup_scheduler(epoch) * optimizer.defaults['lr']\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "    optimizer.step()\n",
        "    if epoch >= warmup_epochs:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / y_val_tensor.size(0) * 100\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        no_improvement_epochs = 0\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    # Early stopping condition\n",
        "    if no_improvement_epochs >= early_stopping_patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Load best model based on validation accuracy\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 5. Test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, test_predicted = torch.max(test_outputs, 1)\n",
        "    test_correct = (test_predicted == y_test_tensor).sum().item()\n",
        "    test_accuracy = test_correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 6. SHAP Analysis\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()\n",
        "\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Plot SHAP summary\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10])"
      ],
      "metadata": {
        "id": "YAXgnNrjqzPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# 1. Dataset generation\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 2. Optimized model with advanced normalization and dropout layers\n",
        "class OptimizedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OptimizedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 12)\n",
        "        self.fc4 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.ln3(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# 3. Training configuration with CLR and patience restart\n",
        "model = OptimizedNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.002, step_size_up=10, mode=\"triangular\")\n",
        "clip_value = 1.0\n",
        "early_stopping_patience = 4\n",
        "\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model = None\n",
        "no_improvement_epochs = 0\n",
        "\n",
        "# 4. Training loop with cyclical LR and early stopping with patience restart\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Cyclical LR adjustment\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / y_val_tensor.size(0) * 100\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        no_improvement_epochs = 0  # Reset if improvement\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    # Early stopping condition with patience restart\n",
        "    if no_improvement_epochs >= early_stopping_patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Load best model based on validation accuracy\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 5. Test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, test_predicted = torch.max(test_outputs, 1)\n",
        "    test_correct = (test_predicted == y_test_tensor).sum().item()\n",
        "    test_accuracy = test_correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 6. SHAP Analysis with focus on high-impact features\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()\n",
        "\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Plot SHAP summary focusing on most impactful features\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10], max_display=5)  # Show top 5 features for impact clarity"
      ],
      "metadata": {
        "id": "XuiFcd4krLXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# 1. Dataset generation\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 2. Optimized model with AGC and Dropconnect regularization\n",
        "class OptimizedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OptimizedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 12)\n",
        "        self.fc4 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.2)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.fc4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln3(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# 3. Training configuration with AGC, SGD with Momentum, and CLR\n",
        "model = OptimizedNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, step_size_up=10, mode=\"triangular\")\n",
        "early_stopping_patience = 4\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0.0\n",
        "best_model = None\n",
        "no_improvement_epochs = 0\n",
        "\n",
        "def adaptive_gradient_clipping(params, clip_value=0.01):\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            if param.grad is not None:\n",
        "                grad_norm = param.grad.norm()\n",
        "                param_norm = param.norm()\n",
        "                max_norm = clip_value * max(param_norm, 1e-6)\n",
        "                if grad_norm > max_norm:\n",
        "                    param.grad.mul_(max_norm / grad_norm)\n",
        "\n",
        "# 4. Training loop with AGC and early stopping with patience restart\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    adaptive_gradient_clipping(model.parameters())\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Cyclical LR adjustment\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_correct = (val_predicted == y_val_tensor).sum().item()\n",
        "        val_accuracy = val_correct / y_val_tensor.size(0) * 100\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        no_improvement_epochs = 0  # Reset if improvement\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    # Early stopping condition with patience restart\n",
        "    if no_improvement_epochs >= early_stopping_patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Load best model based on validation accuracy\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# 5. Test evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, test_predicted = torch.max(test_outputs, 1)\n",
        "    test_correct = (test_predicted == y_test_tensor).sum().item()\n",
        "    test_accuracy = test_correct / y_test_tensor.size(0) * 100\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# 6. SHAP Analysis with focus on high-impact features\n",
        "def model_with_numpy_input(X):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    outputs = model(X_tensor)\n",
        "    return outputs.detach().numpy()\n",
        "\n",
        "explainer = shap.KernelExplainer(model_with_numpy_input, X_train[:100])\n",
        "shap_values = explainer.shap_values(X_test[:10])\n",
        "\n",
        "# Plot SHAP summary focusing on most impactful features\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_test[:10], max_display=5)  # Show top 5 features for impact clarity"
      ],
      "metadata": {
        "id": "Vctv4BnQrkt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkvA-eZ3ZPId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 5  # For early stopping\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the loss curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "traiYHoLwKXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 5  # For early stopping\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PfDuFde8wygl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ykDVDnvx0mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LnkYsyCjyb1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sjMXtDZtzC8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxUMkB17zzGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RE0ZYD0Y0-7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BH5PSZPS6-FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable anomaly detection for debugging\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Define Mixup function\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Define the Advanced Neural Network without in-place operations\n",
        "class AdvancedNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(5, 48)\n",
        "        self.fc2 = nn.Linear(48, 24)\n",
        "        self.fc3 = nn.Linear(24, 24)\n",
        "        self.fc4 = nn.Linear(24, 12)\n",
        "        self.fc5 = nn.Linear(12, 2)\n",
        "        self.ln1 = nn.LayerNorm(48)\n",
        "        self.ln2 = nn.LayerNorm(24)\n",
        "        self.ln3 = nn.LayerNorm(12)\n",
        "        self.dropconnect = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.ln1(self.fc1(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x = torch.relu(self.ln2(self.fc2(x)))  # Non-in-place ReLU\n",
        "        x = self.dropconnect(x)\n",
        "        x_res = x  # Skip connection\n",
        "        x = torch.relu(self.fc3(x))  # Non-in-place ReLU for skip connection\n",
        "        x = x + x_res  # Add skip connection\n",
        "        x = torch.relu(self.ln3(self.fc4(x)))  # Non-in-place ReLU\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "patience = 3  # Early stopping patience\n",
        "accumulation_steps = 2\n",
        "model_save_path = 'best_model.pth'  # Path to save the best model\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, criterion, optimizer, and learning rate scheduler\n",
        "model = AdvancedNN()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Apply label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "# Training and Validation Loop with Early Stopping and Gradient Accumulation\n",
        "best_val_accuracy = 0.0\n",
        "early_stopping_counter = 0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Track loss over training iterations for smoothing\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        loss = loss / accumulation_steps  # Adjust loss for accumulation\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        # Update weights every `accumulation_steps` batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Track average training loss for the epoch\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / len(val_dataset) * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        early_stopping_counter = 0\n",
        "        # Save the model with the best validation accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Saved model with validation accuracy {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Load the best model (with weights_only=True to avoid security warning)\n",
        "model.load_state_dict(torch.load(model_save_path, weights_only=True))\n",
        "\n",
        "# Testing Phase\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / len(test_dataset) * 100\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Optionally, plot the training and validation losses/accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss / Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JgMZUlXK-Y5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Define the custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.targets[idx]\n",
        "\n",
        "        # If there's a transformation, apply it\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define the transformation pipeline: only normalize since data is already tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB images (mean, std)\n",
        "])\n",
        "\n",
        "# Dummy data and targets for demonstration (replace with your actual data)\n",
        "# Let's assume the images are 32x32 RGB images\n",
        "data = np.random.rand(100, 3, 32, 32)  # 100 RGB images of size 32x32\n",
        "targets = np.random.randint(0, 10, size=(100,))  # 100 random class labels (e.g., 10 classes)\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "data = torch.tensor(data, dtype=torch.float32)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "# Create the dataset and dataloaders\n",
        "dataset = CustomDataset(data, targets, transform=transform)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the model (simple CNN example)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 256)  # Adjust this based on input size after pooling\n",
        "        self.fc2 = nn.Linear(256, 10)  # 10 classes for classification\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model = SimpleCNN()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in tqdm(train_loader, desc=\"Training\"):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "mbJy9ar9pbLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data Preprocessing: Set up augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to match VGG input size\n",
        "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # VGG pre-trained normalization\n",
        "])\n",
        "\n",
        "# Load your dataset (replace this with your dataset path and use appropriate Dataset class if needed)\n",
        "train_dataset = datasets.FakeData(transform=transform)  # Example: Use your actual dataset here\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Load pre-trained VGG16 model and modify the classifier\n",
        "model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=10)  # Modify for 10 classes\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Use Adam optimizer with a lower learning rate\n",
        "\n",
        "# Training function\n",
        "def train(model, criterion, optimizer, train_loader, epochs=50):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_preds += labels.size(0)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = (correct_preds / total_preds) * 100\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    return train_losses, train_accuracies\n",
        "\n",
        "# Train the model\n",
        "train_losses, train_accuracies = train(model, criterion, optimizer, train_loader, epochs=50)\n",
        "\n",
        "# Plot training loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss vs Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'vgg16_finetuned.pth')"
      ],
      "metadata": {
        "id": "ZUBK0M_8k5fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "# Data Preprocessing: Set up augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to match VGG input size\n",
        "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
        "    transforms.RandomRotation(10),  # Random rotation\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # VGG pre-trained normalization\n",
        "])\n",
        "\n",
        "# Load your dataset (replace this with your dataset path and appropriate Dataset class if needed)\n",
        "train_dataset = datasets.FakeData(transform=transform)  # Example: Use your actual dataset here\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # Increased batch size\n",
        "\n",
        "# Load pre-trained VGG16 model and modify the classifier\n",
        "model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=10)  # Modify for 10 classes\n",
        "\n",
        "# Freeze initial layers to fine-tune only classifier layers initially\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)  # Scheduler to reduce learning rate\n",
        "\n",
        "# Mixed precision training scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# TensorBoard writer for logging\n",
        "writer = SummaryWriter(log_dir=\"runs/vgg16_finetuning\")\n",
        "\n",
        "# Training function\n",
        "def train(model, criterion, optimizer, train_loader, scheduler, scaler, epochs=50):\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        # Iterate through the training data\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision training: forward pass with autocast\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Step optimizer with scaler\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Track loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_preds += labels.size(0)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "\n",
        "        # Scheduler step at the end of each epoch\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log epoch loss and accuracy\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = (correct_preds / total_preds) * 100\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "        # TensorBoard logging\n",
        "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
        "        writer.add_scalar(\"Accuracy/train\", epoch_accuracy, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "        # Unfreeze layers after 10 epochs for full fine-tuning\n",
        "        if epoch == 9:\n",
        "            for param in model.features.parameters():\n",
        "                param.requires_grad = True\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=0.00001)  # Adjust learning rate for full fine-tuning\n",
        "\n",
        "    return train_losses, train_accuracies\n",
        "\n",
        "# Train the model\n",
        "train_losses, train_accuracies = train(model, criterion, optimizer, train_loader, scheduler, scaler, epochs=50)\n",
        "\n",
        "# Plot training loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss vs Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'vgg16_finetuned.pth')\n",
        "\n",
        "# Close TensorBoard writer\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "rjQdmtGRy8oI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}