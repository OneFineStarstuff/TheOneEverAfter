{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwB3Go1KSC6g+aKv2nMd77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/_Tensor_Network_Expansions_(For_keep_updating).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWhDJU5mfW10"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import opt_einsum as oe\n",
        "import torch\n",
        "\n",
        "\n",
        "class PEPS:\n",
        "    def __init__(self, Lx, Ly, d, D, use_gpu=False):\n",
        "        self.Lx = Lx  # Number of rows\n",
        "        self.Ly = Ly  # Number of columns\n",
        "        self.d = d    # Physical dimension\n",
        "        self.D = D    # Bond dimension\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Initialize tensors\n",
        "        self.tensors = [\n",
        "            [\n",
        "                self.initialize_tensor(d, D)\n",
        "                for _ in range(Ly)\n",
        "            ]\n",
        "            for _ in range(Lx)\n",
        "        ]\n",
        "\n",
        "    def initialize_tensor(self, d, D):\n",
        "        \"\"\"\n",
        "        Initialize a PEPS tensor with shape (physical, left, right, up, down).\n",
        "        \"\"\"\n",
        "        tensor = np.random.randn(d, D, D, D, D)\n",
        "        if self.use_gpu:\n",
        "            tensor = torch.tensor(tensor, dtype=torch.float32).to('cuda')\n",
        "        else:\n",
        "            tensor = torch.tensor(tensor, dtype=torch.float32)\n",
        "        return tensor\n",
        "\n",
        "    def apply_hamiltonian(self, h):\n",
        "        \"\"\"\n",
        "        Apply a local Hamiltonian to each tensor.\n",
        "        \"\"\"\n",
        "        print(f\"Applying Hamiltonian: {h.shape}\")\n",
        "        for i in range(self.Lx):\n",
        "            for j in range(self.Ly):\n",
        "                tensor = self.tensors[i][j]\n",
        "                self.tensors[i][j] = oe.contract(\"ab,bijkl->aijkl\", h, tensor)\n",
        "\n",
        "    def compute_observable(self, observable):\n",
        "        \"\"\"\n",
        "        Compute an observable over the entire PEPS.\n",
        "        \"\"\"\n",
        "        print(f\"Computing observable: {observable.shape}\")\n",
        "        result = 0.0\n",
        "        for row in self.tensors:\n",
        "            for tensor in row:\n",
        "                physical_contracted = oe.contract(\"ab,aijkl->bijkl\", observable, tensor)\n",
        "                result += physical_contracted.sum().item()\n",
        "        return result\n",
        "\n",
        "    def compute_multi_site_observable(self, observable, sites):\n",
        "        \"\"\"\n",
        "        Compute a multi-site observable over specified sites.\n",
        "        \"\"\"\n",
        "        print(f\"Computing multi-site observable on sites: {sites}\")\n",
        "        contracted_tensors = []\n",
        "        for (i, j) in sites:\n",
        "            tensor = self.tensors[i][j]\n",
        "            physical_contracted = oe.contract(\"ab,aijkl->bijkl\", observable, tensor)\n",
        "            contracted_tensors.append(physical_contracted)\n",
        "\n",
        "        result = contracted_tensors[0]\n",
        "        for idx, t in enumerate(contracted_tensors[1:]):\n",
        "            print(f\"Contracting tensor {idx+1}: result.shape={result.shape}, t.shape={t.shape}\")\n",
        "            result, t = self.ensure_bond_match(result, t)\n",
        "            result = oe.contract(\"...ij,...jk->...ik\", result, t)\n",
        "        return result.sum().item()\n",
        "\n",
        "    def full_contraction(self):\n",
        "        \"\"\"\n",
        "        Perform a full contraction of the PEPS.\n",
        "        \"\"\"\n",
        "        print(\"Performing full contraction...\")\n",
        "        row_contractions = []\n",
        "        for row_idx, row in enumerate(self.tensors):\n",
        "            print(f\"Contracting row {row_idx}...\")\n",
        "            row_contracted = self.contract_row(row)\n",
        "            print(f\"Row {row_idx} contraction result: shape={row_contracted.shape}\")\n",
        "            row_contractions.append(row_contracted)\n",
        "\n",
        "        result = row_contractions[0]\n",
        "        for i in range(1, len(row_contractions)):\n",
        "            print(\n",
        "                f\"Contracting rows: result.shape={result.shape}, \"\n",
        "                f\"row_contractions[{i}].shape={row_contractions[i].shape}\"\n",
        "            )\n",
        "            result, row_contractions[i] = self.ensure_bond_match(result, row_contractions[i])\n",
        "            result = oe.contract(\"...ij,...jk->...ik\", result, row_contractions[i])\n",
        "        return result.sum().item()\n",
        "\n",
        "    def contract_row(self, row_tensors):\n",
        "        \"\"\"\n",
        "        Contract tensors in a single row along the horizontal bonds.\n",
        "        \"\"\"\n",
        "        contracted = row_tensors[0]\n",
        "        for idx, t in enumerate(row_tensors[1:]):\n",
        "            print(\n",
        "                f\"Contracting row tensors: contracted.shape={contracted.shape}, \"\n",
        "                f\"t[{idx+1}].shape={t.shape}\"\n",
        "            )\n",
        "            contracted, t = self.ensure_bond_match(contracted, t)\n",
        "            contracted = oe.contract(\"...lm,...mn->...ln\", contracted, t)\n",
        "        return contracted\n",
        "\n",
        "    def ensure_bond_match(self, t1, t2):\n",
        "        \"\"\"\n",
        "        Ensures that bond dimensions of two tensors are aligned via zero-padding.\n",
        "        \"\"\"\n",
        "        def pad_tensor(t, target_shape):\n",
        "            \"\"\"\n",
        "            Zero-pad a tensor to the target shape.\n",
        "            \"\"\"\n",
        "            pad_sizes = [(0, max(0, target_dim - current_dim)) for current_dim, target_dim in zip(t.shape, target_shape)]\n",
        "            pad_sizes = [item for sublist in reversed(pad_sizes) for item in sublist]\n",
        "            return torch.nn.functional.pad(t, pad_sizes)\n",
        "\n",
        "        # Print tensor shapes for debugging\n",
        "        print(f\"Ensuring bond match: t1.shape={t1.shape}, t2.shape={t2.shape}\")\n",
        "\n",
        "        # Pad all dimensions to the maximum shape\n",
        "        max_shape = tuple(max(s1, s2) for s1, s2 in zip(t1.shape, t2.shape))\n",
        "        t1_padded = pad_tensor(t1, max_shape)\n",
        "        t2_padded = pad_tensor(t2, max_shape)\n",
        "\n",
        "        print(f\"After padding: t1_padded.shape={t1_padded.shape}, t2_padded.shape={t2_padded.shape}\")\n",
        "        return t1_padded, t2_padded\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Lattice dimensions\n",
        "    Lx, Ly = 4, 4\n",
        "    # Physical and bond dimensions\n",
        "    d, D = 2, 3\n",
        "    # Check for GPU\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    # Initialize PEPS\n",
        "    peps = PEPS(Lx, Ly, d, D, use_gpu=use_gpu)\n",
        "\n",
        "    # Initialize random Hamiltonian\n",
        "    hamiltonian = np.random.randn(d, d)\n",
        "    if use_gpu:\n",
        "        hamiltonian = torch.tensor(hamiltonian, dtype=torch.float32).to('cuda')\n",
        "    else:\n",
        "        hamiltonian = torch.tensor(hamiltonian, dtype=torch.float32)\n",
        "\n",
        "    # Apply Hamiltonian\n",
        "    peps.apply_hamiltonian(hamiltonian)\n",
        "\n",
        "    # Initialize random observable\n",
        "    observable = np.random.randn(d, d)\n",
        "    if use_gpu:\n",
        "        observable = torch.tensor(observable, dtype=torch.float32).to('cuda')\n",
        "    else:\n",
        "        observable = torch.tensor(observable, dtype=torch.float32)\n",
        "\n",
        "    # Compute single-site observable\n",
        "    observable_result = peps.compute_observable(observable)\n",
        "    print(f\"Observable result: {observable_result}\")\n",
        "\n",
        "    # Compute multi-site observable\n",
        "    multi_site_observable = np.random.randn(d, d)\n",
        "    if use_gpu:\n",
        "        multi_site_observable = torch.tensor(multi_site_observable, dtype=torch.float32).to('cuda')\n",
        "    else:\n",
        "        multi_site_observable = torch.tensor(multi_site_observable, dtype=torch.float32)\n",
        "\n",
        "    multi_site_result = peps.compute_multi_site_observable(multi_site_observable, [(0, 0), (1, 1)])\n",
        "    print(f\"Multi-site observable result: {multi_site_result}\")\n",
        "\n",
        "    # Perform full contraction\n",
        "    result = peps.full_contraction()\n",
        "    print(f\"Full contraction result: {result}\")"
      ]
    }
  ]
}