{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPZ03NKbyYektLBsY+gmcbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Multi_Task_Fine_Tuning_with_Prompt_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXnY81Wver4t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define the FoundationModel class\n",
        "class FoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
        "        super(FoundationModel, self).__init__()\n",
        "        self.model = BertModel.from_pretrained(model_name)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state\n",
        "\n",
        "    def encode_text(self, texts, max_length=128):\n",
        "        encoding = self.tokenizer(texts, padding=True, truncation=True,\n",
        "                                  max_length=max_length, return_tensors=\"pt\")\n",
        "        return encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "# Define the MultiTaskFoundationModel class for multitask learning\n",
        "class MultiTaskFoundationModel(FoundationModel):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", tasks=None):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        # Pass through the transformer\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "    def add_task_tokens(self, texts, task):\n",
        "        # Add task-specific tokens to text\n",
        "        task_texts = [f\"[TASK-{task}] {text}\" for text in texts]\n",
        "        return self.encode_text(task_texts)\n",
        "\n",
        "# Train the multitask model\n",
        "def train_multitask_model(model, train_data, epochs=3, batch_size=32, learning_rate=5e-5):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for task, task_data in train_data.items():\n",
        "            total_loss = 0\n",
        "            train_dataset = TextDataset(task_data, model.tokenizer, for_classification=True)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            for batch in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Task: {task}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n",
        "# Example usage\n",
        "# Assuming train_data is available as a dictionary of tasks with lists of dictionaries containing \"text\" and \"label\"\n",
        "\n",
        "train_data = {\n",
        "    \"task1\": [{\"text\": \"example sentence for task 1\", \"label\": 0}],  # Replace with actual data\n",
        "    \"task2\": [{\"text\": \"example sentence for task 2\", \"label\": 1}]   # Replace with actual data\n",
        "}\n",
        "\n",
        "tasks = {\"task1\": 2, \"task2\": 2}  # Define tasks with number of labels for each\n",
        "\n",
        "# Initialize the multitask model\n",
        "multitask_model = MultiTaskFoundationModel(model_name=\"bert-base-uncased\", tasks=tasks).to(device)\n",
        "\n",
        "# Train the multitask model\n",
        "train_multitask_model(multitask_model, train_data)\n",
        "\n",
        "# Evaluating the multitask model can be done similarly by creating a separate evaluation function"
      ]
    }
  ]
}