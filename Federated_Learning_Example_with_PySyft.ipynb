{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNzvEC8k9XEHUWmIOkuM6W7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Federated_Learning_Example_with_PySyft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install syft"
      ],
      "metadata": {
        "id": "54vdAPEnVG-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade syft"
      ],
      "metadata": {
        "id": "EVFpfnVs3v4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import syft as sy\n",
        "\n",
        "# Step 1: Start a domain node\n",
        "domain = sy.Domain(name=\"My Secure Domain\")\n",
        "\n",
        "# Step 2: Create data and upload it to the domain\n",
        "data = sy.Tensor([1, 2, 3, 4, 5])\n",
        "data_ptr = data.send(domain)  # Send data to the secure domain\n",
        "\n",
        "print(\"Data sent to domain:\", data_ptr)\n",
        "\n",
        "# Step 3: Request access to the data\n",
        "request = data_ptr.request(reason=\"Perform computations securely.\")\n",
        "print(\"Data request created:\", request)\n",
        "\n",
        "# Step 4: Perform operations securely (once access is approved)\n",
        "result = data_ptr + 5  # Example operation\n",
        "print(\"Result of secure computation:\", result)"
      ],
      "metadata": {
        "id": "JvJuFh4htKau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import syft as sy\n",
        "from syft.node.node import Node\n",
        "\n",
        "# Initialize a Node (instead of a Domain)\n",
        "node = Node(name=\"My Secure Node\")\n",
        "\n",
        "# Create a tensor and share it\n",
        "tensor = sy.lib.numpy.array([1, 2, 3, 4])\n",
        "tensor_ptr = tensor.send(node)\n",
        "\n",
        "print(f\"Tensor sent to node: {tensor_ptr}\")"
      ],
      "metadata": {
        "id": "W1sNpL-Gtc9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import syft as sy\n",
        "print(sy.__version__)"
      ],
      "metadata": {
        "id": "51wREqNMtt7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import syft as sy\n",
        "\n",
        "# Create a virtual worker environment\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Create virtual workers representing different clients\n",
        "worker1 = sy.VirtualWorker(hook, id=\"worker1\")\n",
        "worker2 = sy.VirtualWorker(hook, id=\"worker2\")\n",
        "\n",
        "# Define a simple model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Create some dummy data\n",
        "data_worker1 = torch.randn(500, 10)\n",
        "labels_worker1 = torch.randint(0, 2, (500, 1)).float()\n",
        "data_worker2 = torch.randn(500, 10)\n",
        "labels_worker2 = torch.randint(0, 2, (500, 1)).float()\n",
        "\n",
        "# Create datasets and loaders for each worker\n",
        "dataset_worker1 = TensorDataset(data_worker1, labels_worker1)\n",
        "dataset_worker2 = TensorDataset(data_worker2, labels_worker2)\n",
        "dataloader_worker1 = DataLoader(dataset_worker1, batch_size=32, shuffle=True)\n",
        "dataloader_worker2 = DataLoader(dataset_worker2, batch_size=32, shuffle=True)\n",
        "\n",
        "# Federate the datasets\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    dataset_worker1.federate((worker1, worker2)), batch_size=32\n",
        ")\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = MyModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Example for 10 epochs\n",
        "    total_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
        "        model.send(data.location)  # Send the model to the right worker\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.get().item()\n",
        "        model.get()  # Get the model back\n",
        "\n",
        "    avg_loss = total_loss / len(federated_train_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "jxuwEZEDU__c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import syft as sy\n",
        "\n",
        "# Initialize the Syft context\n",
        "syft_context = sy.CoreContext()\n",
        "\n",
        "# Create virtual workers representing different clients\n",
        "worker1 = sy.VirtualMachine(name=\"worker1\")\n",
        "worker2 = sy.VirtualMachine(name=\"worker2\")\n",
        "\n",
        "client1 = worker1.get_root_client()\n",
        "client2 = worker2.get_root_client()\n",
        "\n",
        "# Define a simple model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Create some dummy data\n",
        "data_worker1 = torch.randn(500, 10)\n",
        "labels_worker1 = torch.randint(0, 2, (500, 1)).float()\n",
        "data_worker2 = torch.randn(500, 10)\n",
        "labels_worker2 = torch.randint(0, 2, (500, 1)).float()\n",
        "\n",
        "# Create datasets and loaders for each worker\n",
        "dataset_worker1 = TensorDataset(data_worker1, labels_worker1)\n",
        "dataset_worker2 = TensorDataset(data_worker2, labels_worker2)\n",
        "\n",
        "# Send data to virtual workers\n",
        "remote_dataset_worker1 = sy.lib.python.List(\n",
        "    [data_worker1.send(client1), labels_worker1.send(client1)]\n",
        ")\n",
        "remote_dataset_worker2 = sy.lib.python.List(\n",
        "    [data_worker2.send(client2), labels_worker2.send(client2)]\n",
        ")\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = MyModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Example for 10 epochs\n",
        "    total_loss = 0.0\n",
        "    for client, remote_dataset in [(client1, remote_dataset_worker1), (client2, remote_dataset_worker2)]:\n",
        "        # Send model to the worker\n",
        "        model_ptr = model.send(client)\n",
        "\n",
        "        # Access the remote data\n",
        "        data_ptr, target_ptr = remote_dataset[0], remote_dataset[1]\n",
        "\n",
        "        # Perform training on the remote worker\n",
        "        optimizer.zero_grad()\n",
        "        output_ptr = model_ptr(data_ptr)\n",
        "        loss_ptr = loss_fn(output_ptr, target_ptr)\n",
        "        loss_ptr.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the loss back\n",
        "        loss = loss_ptr.get()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Get the model back\n",
        "        model.get()\n",
        "\n",
        "    avg_loss = total_loss / 2  # Since we have 2 workers\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "yZQ2A1ZDsGUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the hook\n",
        "hook = sy.TorchHook(torch)  # Hook PyTorch to use PySyft\n",
        "\n",
        "# Create virtual workers\n",
        "worker1 = sy.VirtualWorker(hook, id=\"worker1\")\n",
        "worker2 = sy.VirtualWorker(hook, id=\"worker2\")\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model = SimpleNN()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Simulate some local training data for each worker\n",
        "data_worker1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]], requires_grad=True)\n",
        "target_worker1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "\n",
        "data_worker2 = torch.tensor([[5.0], [6.0], [7.0], [8.0]], requires_grad=True)\n",
        "target_worker2 = torch.tensor([[5.0], [6.0], [7.0], [8.0]])\n",
        "\n",
        "# Send the data to the workers\n",
        "data_worker1 = data_worker1.send(worker1)\n",
        "target_worker1 = target_worker1.send(worker1)\n",
        "\n",
        "data_worker2 = data_worker2.send(worker2)\n",
        "target_worker2 = target_worker2.send(worker2)\n",
        "\n",
        "# Train the model on each worker and perform federated averaging\n",
        "\n",
        "def train_on_worker(worker, model, data, target, optimizer, loss_fn):\n",
        "    model.send(worker)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    prediction = model(data)\n",
        "    loss = loss_fn(prediction, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get the updated model back from the worker\n",
        "    model.get()\n",
        "\n",
        "# Train the model on each worker\n",
        "train_on_worker(worker1, model, data_worker1, target_worker1, optimizer, loss_fn)\n",
        "train_on_worker(worker2, model, data_worker2, target_worker2, optimizer, loss_fn)\n",
        "\n",
        "# After training, the model parameters are updated on both workers. You can now perform federated averaging.\n",
        "def federated_avg(models):\n",
        "    # Averaging the parameters of the models\n",
        "    model_params = [model.state_dict() for model in models]\n",
        "\n",
        "    avg_params = {}\n",
        "    for param in model_params[0]:\n",
        "        avg_params[param] = torch.stack([model_param[param].float() for model_param in model_params]).mean(0)\n",
        "\n",
        "    # Set the averaged parameters to the original model\n",
        "    model.load_state_dict(avg_params)\n",
        "\n",
        "# Get the models after training on each worker\n",
        "model_worker1 = model.copy().send(worker1)\n",
        "model_worker2 = model.copy().send(worker2)\n",
        "\n",
        "# Federated averaging\n",
        "federated_avg([model_worker1, model_worker2])\n",
        "\n",
        "# Now the model has the averaged parameters from both workers"
      ],
      "metadata": {
        "id": "rTVQbKRZ3oYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDXrnvLY5KZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import syft as sy\n",
        "\n",
        "# Initialize the hook (This is the updated method for initializing PySyft)\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Create virtual workers\n",
        "worker1 = sy.VirtualWorker(hook, id=\"worker1\")\n",
        "worker2 = sy.VirtualWorker(hook, id=\"worker2\")\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model = SimpleNN()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Simulate some local training data for each worker\n",
        "data_worker1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]], requires_grad=True)\n",
        "target_worker1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "\n",
        "data_worker2 = torch.tensor([[5.0], [6.0], [7.0], [8.0]], requires_grad=True)\n",
        "target_worker2 = torch.tensor([[5.0], [6.0], [7.0], [8.0]])\n",
        "\n",
        "# Send the data to the workers\n",
        "data_worker1 = data_worker1.send(worker1)\n",
        "target_worker1 = target_worker1.send(worker1)\n",
        "\n",
        "data_worker2 = data_worker2.send(worker2)\n",
        "target_worker2 = target_worker2.send(worker2)\n",
        "\n",
        "# Train the model on each worker and perform federated averaging\n",
        "\n",
        "def train_on_worker(worker, model, data, target, optimizer, loss_fn):\n",
        "    model.send(worker)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    prediction = model(data)\n",
        "    loss = loss_fn(prediction, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get the updated model back from the worker\n",
        "    model.get()\n",
        "\n",
        "# Train the model on each worker\n",
        "train_on_worker(worker1, model, data_worker1, target_worker1, optimizer, loss_fn)\n",
        "train_on_worker(worker2, model, data_worker2, target_worker2, optimizer, loss_fn)\n",
        "\n",
        "# After training, the model parameters are updated on both workers. You can now perform federated averaging.\n",
        "def federated_avg(models):\n",
        "    # Averaging the parameters of the models\n",
        "    model_params = [model.state_dict() for model in models]\n",
        "\n",
        "    avg_params = {}\n",
        "    for param in model_params[0]:\n",
        "        avg_params[param] = torch.stack([model_param[param].float() for model_param in model_params]).mean(0)\n",
        "\n",
        "    # Set the averaged parameters to the original model\n",
        "    model.load_state_dict(avg_params)\n",
        "\n",
        "# Get the models after training on each worker\n",
        "model_worker1 = model.copy().send(worker1)\n",
        "model_worker2 = model.copy().send(worker2)\n",
        "\n",
        "# Federated averaging\n",
        "federated_avg([model_worker1, model_worker2])\n",
        "\n",
        "# Now the model has the averaged parameters from both workers"
      ],
      "metadata": {
        "id": "l3RD98Dk5L7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the hook\n",
        "hook = sy.TorchHook(torch)  # Use TorchHook to hook PyTorch to PySyft\n",
        "\n",
        "# Create virtual workers\n",
        "worker1 = sy.VirtualWorker(hook, id=\"worker1\")\n",
        "worker2 = sy.VirtualWorker(hook, id=\"worker2\")\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model = SimpleNN()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Simulate some local training data for each worker\n",
        "data_worker1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]], requires_grad=True)\n",
        "target_worker1 = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "\n",
        "data_worker2 = torch.tensor([[5.0], [6.0], [7.0], [8.0]], requires_grad=True)\n",
        "target_worker2 = torch.tensor([[5.0], [6.0], [7.0], [8.0]])\n",
        "\n",
        "# Send the data to the workers\n",
        "data_worker1 = data_worker1.send(worker1)\n",
        "target_worker1 = target_worker1.send(worker1)\n",
        "\n",
        "data_worker2 = data_worker2.send(worker2)\n",
        "target_worker2 = target_worker2.send(worker2)\n",
        "\n",
        "# Train the model on each worker and perform federated averaging\n",
        "\n",
        "def train_on_worker(worker, model, data, target, optimizer, loss_fn):\n",
        "    model.send(worker)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    prediction = model(data)\n",
        "    loss = loss_fn(prediction, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get the updated model back from the worker\n",
        "    model.get()\n",
        "\n",
        "# Train the model on each worker\n",
        "train_on_worker(worker1, model, data_worker1, target_worker1, optimizer, loss_fn)\n",
        "train_on_worker(worker2, model, data_worker2, target_worker2, optimizer, loss_fn)\n",
        "\n",
        "# After training, the model parameters are updated on both workers. You can now perform federated averaging.\n",
        "def federated_avg(models):\n",
        "    # Averaging the parameters of the models\n",
        "    model_params = [model.state_dict() for model in models]\n",
        "\n",
        "    avg_params = {}\n",
        "    for param in model_params[0]:\n",
        "        avg_params[param] = torch.stack([model_param[param].float() for model_param in model_params]).mean(0)\n",
        "\n",
        "    # Set the averaged parameters to the original model\n",
        "    model.load_state_dict(avg_params)\n",
        "\n",
        "# Get the models after training on each worker\n",
        "model_worker1 = model.copy().send(worker1)\n",
        "model_worker2 = model.copy().send(worker2)\n",
        "\n",
        "# Federated averaging\n",
        "federated_avg([model_worker1, model_worker2])\n",
        "\n",
        "# Now the model has the averaged parameters from both workers"
      ],
      "metadata": {
        "id": "F0mL6k8k5JFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}