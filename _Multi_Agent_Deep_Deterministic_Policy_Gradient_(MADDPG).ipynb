{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMBPupxc1dziqglsGExaYEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/_Multi_Agent_Deep_Deterministic_Policy_Gradient_(MADDPG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Define Actor Network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.l1(x))\n",
        "        x = torch.relu(self.l2(x))\n",
        "        x = torch.tanh(self.l3(x)) * self.max_action\n",
        "        return x\n",
        "\n",
        "# Define Critic Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        x = torch.cat([x, u], dim=1)\n",
        "        x = torch.relu(self.l1(x))\n",
        "        x = torch.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "        return x\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=int(1e6)):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
        "\n",
        "# MADDPG Algorithm\n",
        "class MADDPG:\n",
        "    def __init__(self, state_dim, action_dim, max_action, num_agents):\n",
        "        self.actors = [Actor(state_dim, action_dim, max_action).to(device) for _ in range(num_agents)]\n",
        "        self.actors_target = [Actor(state_dim, action_dim, max_action).to(device) for _ in range(num_agents)]\n",
        "        self.critics = [Critic(state_dim*num_agents, action_dim*num_agents).to(device) for _ in range(num_agents)]\n",
        "        self.critics_target = [Critic(state_dim*num_agents, action_dim*num_agents).to(device) for _ in range(num_agents)]\n",
        "\n",
        "        self.actor_optimizers = [optim.Adam(actor.parameters(), lr=1e-4) for actor in self.actors]\n",
        "        self.critic_optimizers = [optim.Adam(critic.parameters(), lr=1e-3) for critic in self.critics]\n",
        "\n",
        "        for i in range(num_agents):\n",
        "            self.actors_target[i].load_state_dict(self.actors[i].state_dict())\n",
        "            self.critics_target[i].load_state_dict(self.critics[i].state_dict())\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        self.max_action = max_action\n",
        "        self.num_agents = num_agents\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "\n",
        "    def select_action(self, state):\n",
        "        actions = []\n",
        "        for i in range(self.num_agents):\n",
        "            state_tensor = torch.FloatTensor(state[i]).unsqueeze(0).to(device)\n",
        "            action = self.actors[i](state_tensor).cpu().data.numpy().flatten()\n",
        "            actions.append(action)\n",
        "        return actions\n",
        "\n",
        "    def train(self, batch_size=100):\n",
        "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        next_state = torch.FloatTensor(next_state).to(device)\n",
        "        action = torch.FloatTensor(action).to(device)\n",
        "        reward = torch.FloatTensor(reward).to(device)\n",
        "        done = torch.FloatTensor(done).to(device)\n",
        "\n",
        "        for i in range(self.num_agents):\n",
        "            current_states = torch.cat([state[:, j, :] for j in range(self.num_agents)], dim=1)\n",
        "            current_actions = torch.cat([action[:, j, :] for j in range(self.num_agents)], dim=1)\n",
        "            next_states = torch.cat([next_state[:, j, :] for j in range(self.num_agents)], dim=1)\n",
        "\n",
        "            # Target Critic\n",
        "            with torch.no_grad():\n",
        "                target_actions = [self.actors_target[j](next_state[:, j, :]) for j in range(self.num_agents)]\n",
        "                target_actions = torch.cat(target_actions, dim=1)\n",
        "                target_Q = reward[:, i].unsqueeze(1) + self.gamma * self.critics_target[i](next_states, target_actions) * (1 - done[:, i].unsqueeze(1))\n",
        "\n",
        "            current_Q = self.critics[i](current_states, current_actions)\n",
        "            critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
        "\n",
        "            self.critic_optimizers[i].zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizers[i].step()\n",
        "\n",
        "            actor_actions = current_actions.clone()\n",
        "            actor_actions[:, i*action.shape[2]:(i+1)*action.shape[2]] = self.actors[i](state[:, i, :])\n",
        "            actor_loss = -self.critics[i](current_states, actor_actions).mean()\n",
        "\n",
        "            self.actor_optimizers[i].zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizers[i].step()\n",
        "\n",
        "            # Update target networks\n",
        "            for param, target_param in zip(self.critics[i].parameters(), self.critics_target[i].parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for param, target_param in zip(self.actors[i].parameters(), self.actors_target[i].parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "# Example usage of the MADDPG algorithm in a simulated environment\n",
        "if __name__ == \"__main__\":\n",
        "    state_dim = 24  # Example state dimension\n",
        "    action_dim = 4  # Example action dimension\n",
        "    max_action = 1  # Maximum action value\n",
        "    num_agents = 3  # Number of agents\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    maddpg = MADDPG(state_dim, action_dim, max_action, num_agents)\n",
        "\n",
        "    # Example training loop (replace with real environment interaction)\n",
        "    for episode in range(1000):\n",
        "        states = [np.random.randn(state_dim) for _ in range(num_agents)]\n",
        "        for t in range(100):\n",
        "            actions = maddpg.select_action(states)\n",
        "            next_states = [np.random.randn(state_dim) for _ in range(num_agents)]\n",
        "            rewards = [np.random.randn() for _ in range(num_agents)]\n",
        "            dones = [False for _ in range(num_agents)]\n",
        "\n",
        "            maddpg.replay_buffer.add(states, actions, rewards, next_states, dones)\n",
        "\n",
        "            if len(maddpg.replay_buffer.buffer) > 1000:\n",
        "                maddpg.train()\n",
        "\n",
        "            states = next_states\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f'Episode {episode} completed.')\n",
        "\n",
        "    print(\"MADDPG training completed.\")"
      ],
      "metadata": {
        "id": "t3zLPnsWEJQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}