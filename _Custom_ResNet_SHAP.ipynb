{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOPd4gA9Zq4CH5KOsIH67/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Custom_ResNet_SHAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom BasicBlock to avoid in-place operations\n",
        "class CustomBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):\n",
        "        super(CustomBasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out.clone(), inplace=False)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x.clone())\n",
        "\n",
        "        out = out.clone() + identity  # Clone before addition to avoid in-place modification\n",
        "        out = F.relu(out.clone(), inplace=False)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Custom ResNet using CustomBasicBlock\n",
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, groups=1, base_width=64, dilation=1, norm_layer=norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=1, base_width=64, dilation=1, norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x.clone())  # Clone to avoid in-place operation\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x.clone())  # Clone to avoid in-place operation\n",
        "        x = self.layer2(x.clone())  # Clone to avoid in-place operation\n",
        "        x = self.layer3(x.clone())  # Clone to avoid in-place operation\n",
        "        x = self.layer4(x.clone())  # Clone to avoid in-place operation\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x.clone())  # Clone to avoid in-place operation\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the custom model with pre-trained weights\n",
        "model = CustomResNet(CustomBasicBlock, [2, 2, 2, 2])\n",
        "state_dict = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).state_dict()\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Initialize SHAP explainer with the custom model\n",
        "explainer = shap.DeepExplainer(model, torch.randn(1, 3, 224, 224))\n",
        "\n",
        "# Generate SHAP values for an input image\n",
        "sample_image = torch.randn(1, 3, 224, 224)\n",
        "shap_values = explainer.shap_values(sample_image, check_additivity=False)\n",
        "\n",
        "# Convert SHAP values and sample image to numpy for SHAP visualization\n",
        "shap_values_class_0 = shap_values[0][0]  # Extract SHAP values for the first class\n",
        "sample_image_np = sample_image.squeeze().permute(1, 2, 0).detach().numpy()\n",
        "\n",
        "# Normalize sample image and SHAP values to range [0, 1] for visualization\n",
        "sample_image_np = np.clip(sample_image_np, 0, 1)\n",
        "shap_min, shap_max = shap_values_class_0.min(), shap_values_class_0.max()\n",
        "shap_values_class_0 = (shap_values_class_0 - shap_min) / (shap_max - shap_min)\n",
        "\n",
        "# Ensure both `sample_image_np` and `shap_values_class_0` are NumPy arrays with correct shapes for image_plot\n",
        "sample_image_np = np.array([sample_image_np])  # Add batch dimension for SHAP\n",
        "shap_values_class_0 = np.array([shap_values_class_0])  # Add batch dimension for SHAP\n",
        "\n",
        "# Visualize SHAP values for the first class\n",
        "shap.image_plot(shap_values_class_0, sample_image_np)"
      ],
      "metadata": {
        "id": "BZglvCx8MJrV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}