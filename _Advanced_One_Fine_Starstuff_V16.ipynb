{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNfCBgpsrp5j6qmiSj/oX2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Advanced_One_Fine_Starstuff_V16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "\n",
        "# Transformations with Mixup and CutMix as options\n",
        "class MixupCutMixDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, alpha=1.0, use_mixup=True):\n",
        "        self.dataset = dataset\n",
        "        self.alpha = alpha\n",
        "        self.use_mixup = use_mixup\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "        if self.alpha > 0 and np.random.rand() < 0.5:\n",
        "            # Mixup or CutMix\n",
        "            idx2 = np.random.randint(0, len(self.dataset))\n",
        "            img2, label2 = self.dataset[idx2]\n",
        "            if self.use_mixup:\n",
        "                lam = np.random.beta(self.alpha, self.alpha)\n",
        "                img = lam * img + (1 - lam) * img2\n",
        "                label = lam * label + (1 - lam) * label2\n",
        "            else:\n",
        "                lam = np.random.beta(self.alpha, self.alpha)\n",
        "                bbx1, bby1, bbx2, bby2 = self.rand_bbox(img.size(), lam)\n",
        "                img[:, bbx1:bbx2, bby1:bby2] = img2[:, bbx1:bbx2, bby1:bby2]\n",
        "        return img, label\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[1]\n",
        "        H = size[2]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = int(W * cut_rat)\n",
        "        cut_h = int(H * cut_rat)\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# Define the CNN model with Adaptive Gradient Clipping (AGC)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 8 * 8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20, use_amp=True):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    scaler = GradScaler(\"cuda\") if use_amp else None\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.long)  # Ensure labels are of Long type\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(\"cuda\", enabled=use_amp):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            if use_amp:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        acc = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {acc:.2f}%\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.long)  # Ensure labels are of Long type\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Argument parsing for training configurations\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Advanced CIFAR-10 Training')\n",
        "    parser.add_argument('--num_epochs', type=int, default=30)\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.001)\n",
        "    parser.add_argument('--batch_size', type=int, default=64)\n",
        "    parser.add_argument('--use_amp', action='store_true', default=True)\n",
        "    parser.add_argument('--alpha', type=float, default=1.0, help='Mixup/CutMix alpha')\n",
        "    parser.add_argument('--optimizer', type=str, default='AdamW', help='Optimizer (AdamW or SGD)')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    # Model, criterion, optimizer, and scheduler setup\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "    ])\n",
        "\n",
        "    train_dataset = MixupCutMixDataset(CIFAR10(root='./data', train=True, download=True, transform=train_transform), alpha=args.alpha, use_mixup=True)\n",
        "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    model = SimpleCNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if args.optimizer == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=5e-4)\n",
        "    elif args.optimizer == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "    logging.info(\"Starting advanced training with Mixup/CutMix, AMP, and CosineAnnealingWarmRestarts...\")\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=args.num_epochs, use_amp=args.use_amp)\n",
        "    logging.info(\"Training completed. Best model saved as 'best_model.pth'.\")"
      ],
      "metadata": {
        "id": "EY1jDB9c_AgO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}