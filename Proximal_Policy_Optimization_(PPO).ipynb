{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyONVu+tA+tP8jxvVKd96AUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/Proximal_Policy_Optimization_(PPO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Define the actor-critic network architecture\n",
        "class PPOActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PPOActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.actor = nn.Linear(256, action_dim)\n",
        "        self.critic = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        policy = torch.softmax(self.actor(x), dim=-1)\n",
        "        value = self.critic(x)\n",
        "        return policy, value\n",
        "\n",
        "# Define the PPO algorithm\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, K_epochs=4):\n",
        "        self.policy = PPOActorCritic(state_dim, action_dim).to(device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        self.policy_old = PPOActorCritic(state_dim, action_dim).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        with torch.no_grad():\n",
        "            policy, _ = self.policy_old(state)\n",
        "        action_prob = Categorical(policy)\n",
        "        action = action_prob.sample()\n",
        "        return action.item(), action_prob.log_prob(action)\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        policy, value = self.policy(state)\n",
        "        action_prob = Categorical(policy)\n",
        "        action_logprobs = action_prob.log_prob(action)\n",
        "        dist_entropy = action_prob.entropy()\n",
        "        return action_logprobs, torch.squeeze(value), dist_entropy\n",
        "\n",
        "    def update(self, memory):\n",
        "        states = torch.FloatTensor(memory.states).to(device)\n",
        "        actions = torch.LongTensor(memory.actions).to(device)\n",
        "        logprobs = torch.FloatTensor(memory.logprobs).to(device)\n",
        "        rewards = torch.FloatTensor(memory.rewards).to(device)\n",
        "        dones = torch.FloatTensor(memory.dones).to(device)\n",
        "\n",
        "        returns = []\n",
        "        discounted_return = 0\n",
        "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "            if done:\n",
        "                discounted_return = 0\n",
        "            discounted_return = reward + (self.gamma * discounted_return)\n",
        "            returns.insert(0, discounted_return)\n",
        "\n",
        "        returns = torch.tensor(returns).to(device)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
        "\n",
        "        for _ in range(self.K_epochs):\n",
        "            logprobs_new, state_values, dist_entropy = self.evaluate(states, actions)\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            ratios = torch.exp(logprobs_new - logprobs.detach())\n",
        "            advantages = returns - state_values.detach()\n",
        "\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, returns) - 0.01 * dist_entropy\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "# Memory class to store experiences\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.logprobs.clear()\n",
        "        self.rewards.clear()\n",
        "        self.dones.clear()\n",
        "\n",
        "# Training function for PPO\n",
        "def train_ppo(env_name, max_episodes=1000):\n",
        "    env = gym.make(env_name)  # Create the environment\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    memory = Memory()\n",
        "    ppo = PPO(state_dim, action_dim)\n",
        "\n",
        "    # Initialize TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    running_reward = 0\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()  # Reset the environment and get initial state\n",
        "\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(1000):\n",
        "            action, logprob = ppo.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            memory.states.append(state)\n",
        "            memory.actions.append(action)\n",
        "            memory.logprobs.append(logprob.item())\n",
        "            memory.rewards.append(reward)\n",
        "            memory.dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        ppo.update(memory)\n",
        "        memory.clear_memory()\n",
        "\n",
        "        running_reward += episode_reward\n",
        "        avg_reward = running_reward / (episode + 1)\n",
        "\n",
        "        # Log average reward to TensorBoard\n",
        "        writer.add_scalar('Average Reward', avg_reward, episode)\n",
        "\n",
        "        print(f'Episode {episode}, Average Reward: {avg_reward}')\n",
        "\n",
        "    writer.close()\n",
        "    env.close()\n",
        "\n",
        "# Define your device (GPU or CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train PPO on CartPole-v1\n",
        "train_ppo(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "MpL1o5YuyuT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}