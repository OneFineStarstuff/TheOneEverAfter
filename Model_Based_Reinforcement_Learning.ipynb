{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMdQVty96ki94vE0bacHZ9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/Model_Based_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD57Lkv-Cokc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Transition model definition\n",
        "class TransitionModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(TransitionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)  # First layer\n",
        "        self.fc2 = nn.Linear(256, 256)                      # Second layer\n",
        "        self.fc_next_state = nn.Linear(256, state_dim)     # Output layer for next state\n",
        "        self.fc_reward = nn.Linear(256, 1)                 # Output layer for reward\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=-1)             # Concatenate state and action\n",
        "        x = torch.relu(self.fc1(x))                         # First hidden layer with ReLU activation\n",
        "        x = torch.relu(self.fc2(x))                         # Second hidden layer with ReLU activation\n",
        "        next_state = self.fc_next_state(x)                 # Predict next state\n",
        "        reward = self.fc_reward(x)                           # Predict reward\n",
        "        return next_state, reward\n",
        "\n",
        "# Simple model definition (for action selection)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, action_dim)          # Map states to actions\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Function to select an action based on model predictions with exploration\n",
        "def select_action(model, transition_model, state, action_space, epsilon=0.1):\n",
        "    if np.random.rand() < epsilon:  # Exploration: select a random action\n",
        "        return action_space[np.random.choice(len(action_space))]\n",
        "\n",
        "    best_action = None\n",
        "    best_predicted_reward = -float('inf')\n",
        "\n",
        "    # Evaluate each action in the action space\n",
        "    for action in action_space:\n",
        "        action_tensor = action.clone().detach().unsqueeze(0)  # Prepare action tensor\n",
        "        _, pred_reward = transition_model(state, action_tensor)  # Get predicted reward\n",
        "\n",
        "        if pred_reward.item() > best_predicted_reward:  # Update best action if needed\n",
        "            best_predicted_reward = pred_reward.item()\n",
        "            best_action = action\n",
        "\n",
        "    return best_action\n",
        "\n",
        "# Training function for the transition model\n",
        "def train(transition_model, model, optimizer, episodes=1000):\n",
        "    for episode in range(episodes):\n",
        "        current_state = torch.randn(1, state_dim)  # Simulate a random initial state\n",
        "\n",
        "        # Define the discrete action space (random actions for simplicity)\n",
        "        action_space = [torch.randn(action_dim) for _ in range(10)]\n",
        "\n",
        "        # Select an optimal action using the transition model\n",
        "        optimal_action = select_action(model, transition_model, current_state, action_space)\n",
        "\n",
        "        # Get the next state and reward from the transition model\n",
        "        next_state, reward = transition_model(current_state, optimal_action.unsqueeze(0))\n",
        "\n",
        "        # Compute loss (negative reward as loss for minimization)\n",
        "        loss = -reward.mean()\n",
        "\n",
        "        # Backpropagation to update model weights\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()       # Backpropagate loss\n",
        "        optimizer.step()      # Update weights\n",
        "\n",
        "        if episode % 100 == 0:  # Log every 100 episodes\n",
        "            print(f'Episode {episode}, Loss: {loss.item()}')\n",
        "\n",
        "# Example usage of the framework\n",
        "if __name__ == \"__main__\":\n",
        "    state_dim = 10  # Dimension of the state space\n",
        "    action_dim = 4  # Dimension of the action space\n",
        "\n",
        "    # Initialize models and optimizer\n",
        "    transition_model = TransitionModel(state_dim, action_dim)\n",
        "    model = SimpleModel(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(transition_model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the transition model\n",
        "    train(transition_model, model, optimizer)\n",
        "\n",
        "    # Simulate a final selection of optimal action after training\n",
        "    current_state = torch.randn(1, state_dim)\n",
        "    action_space = [torch.randn(action_dim) for _ in range(10)]\n",
        "    optimal_action = select_action(model, transition_model, current_state, action_space)\n",
        "\n",
        "    print(f'Optimal Action after training: {optimal_action}')"
      ]
    }
  ]
}