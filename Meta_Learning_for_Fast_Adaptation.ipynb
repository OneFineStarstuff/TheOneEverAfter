{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOubeC6FjMwzt0Q7H9qsAj7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Meta_Learning_for_Fast_Adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from transformers import AdamW as TransformersAdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "# Download NLTK wordnet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cpu\")  # Switch to CPU to reduce memory usage\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=64, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define the MAML model class\n",
        "class MAMLModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state[:, 0, :]  # Use CLS token\n",
        "\n",
        "    def clone_parameters(self):\n",
        "        return {name: param.clone() for name, param in self.named_parameters()}\n",
        "\n",
        "    def fast_adapt(self, support_data, query_data, optimizer, n_steps=5, lr_inner=0.01):\n",
        "        original_params = self.clone_parameters()\n",
        "        for _ in range(n_steps):\n",
        "            support_input, support_attention, support_target = support_data\n",
        "            optimizer.zero_grad()\n",
        "            logits = self(support_input, support_attention)\n",
        "            loss = F.cross_entropy(logits, support_target)\n",
        "            loss.backward()\n",
        "\n",
        "            for name, param in self.named_parameters():\n",
        "                if param.grad is not None:  # Check for None gradients\n",
        "                    param.data -= lr_inner * param.grad\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        query_input, query_attention, query_target = query_data\n",
        "        query_logits = self(query_input, query_attention)\n",
        "        query_loss = F.cross_entropy(query_logits, query_target)\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            param.data = original_params[name]  # Restore original parameters\n",
        "\n",
        "        return query_loss\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Augmenting the dataset with more examples and synonym replacement\n",
        "texts = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"label\": 0},\n",
        "    {\"text\": \"A journey of a thousand miles begins with a single step.\", \"label\": 0},\n",
        "    {\"text\": \"To be or not to be, that is the question.\", \"label\": 0},\n",
        "    {\"text\": \"All that glitters is not gold.\", \"label\": 0},\n",
        "    {\"text\": \"The early bird catches the worm.\", \"label\": 1},\n",
        "    {\"text\": \"A picture is worth a thousand words.\", \"label\": 1},\n",
        "    {\"text\": \"Better late than never.\", \"label\": 1},\n",
        "    {\"text\": \"Actions speak louder than words.\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Augmenting data with synonyms\n",
        "augmented_texts = []\n",
        "for text in texts:\n",
        "    for _ in range(3):  # Create 3 augmented versions of each sentence\n",
        "        augmented_text = synonym_replacement(text[\"text\"])\n",
        "        augmented_texts.append({\"text\": augmented_text, \"label\": text[\"label\"]})\n",
        "texts.extend(augmented_texts)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(texts)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data, tokenizer, for_classification=True)\n",
        "val_dataset = TextDataset(val_data, tokenizer, for_classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Initialize Longformer model\n",
        "base_model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "maml_model = MAMLModel(base_model).to(device)\n",
        "optimizer = TransformersAdamW(maml_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train the MAML model\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    for support_batch, query_batch in zip(train_dataloader, val_dataloader):\n",
        "        support_input, support_attention, support_target = support_batch\n",
        "        query_input, query_attention, query_target = query_batch\n",
        "\n",
        "        support_input, support_attention, support_target = support_input.to(device), support_attention.to(device), support_target.to(device)\n",
        "        query_input, query_attention, query_target = query_input.to(device), query_attention.to(device), query_target.to(device)\n",
        "\n",
        "        support_data = (support_input, support_attention, support_target)\n",
        "        query_data = (query_input, query_attention, query_target)\n",
        "\n",
        "        query_loss = maml_model.fast_adapt(support_data, query_data, optimizer)\n",
        "        print(f\"Epoch {epoch + 1}, Query Loss: {query_loss.item()}\")"
      ],
      "metadata": {
        "id": "5vMob3ohMLCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "rwNEgY-9maOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KpKP8h3k7ox"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=2048, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = task_data\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Augmenting the dataset with more examples and synonym replacement\n",
        "texts = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"label\": 0},\n",
        "    {\"text\": \"A journey of a thousand miles begins with a single step.\", \"label\": 0},\n",
        "    {\"text\": \"To be or not to be, that is the question.\", \"label\": 0},\n",
        "    {\"text\": \"All that glitters is not gold.\", \"label\": 0},\n",
        "    {\"text\": \"The early bird catches the worm.\", \"label\": 1},\n",
        "    {\"text\": \"A picture is worth a thousand words.\", \"label\": 1},\n",
        "    {\"text\": \"Better late than never.\", \"label\": 1},\n",
        "    {\"text\": \"Actions speak louder than words.\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Augmenting data with synonyms\n",
        "augmented_texts = []\n",
        "for text in texts:\n",
        "    for _ in range(3):  # Create 3 augmented versions of each sentence\n",
        "        augmented_text = synonym_replacement(text[\"text\"])\n",
        "        augmented_texts.append({\"text\": augmented_text, \"label\": text[\"label\"]})\n",
        "texts.extend(augmented_texts)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(texts)\n",
        "\n",
        "# Split data into training and validation sets again\n",
        "train_data, val_data = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data, tokenizer, for_classification=True)\n",
        "val_dataset = TextDataset(val_data, tokenizer, for_classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Train model again with the augmented dataset\n",
        "model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks={\"classification\": 3, \"sentiment\": 2}).to(device)\n",
        "train_with_scheduler(model, {\"classification\": train_dataset, \"sentiment\": train_dataset}, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "# Evaluate model\n",
        "for task, dataset in {\"classification\": val_dataset, \"sentiment\": val_dataset}.items():\n",
        "    print(f\"Evaluating task: {task}\")\n",
        "    evaluate_with_metrics(model, dataset, task)\n",
        "\n",
        "# Define MAML Model\n",
        "class MAMLModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids, attention_mask).logits\n",
        "\n",
        "    def clone_parameters(self):\n",
        "        return {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "    def fast_adapt(self, support_data, query_data, optimizer, n_steps=5, lr_inner=0.01):\n",
        "        # Support step\n",
        "        original_params = self.clone_parameters()\n",
        "        for _ in range(n_steps):\n",
        "            support_input, support_attention, support_target = support_data\n",
        "            optimizer.zero_grad()\n",
        "            logits = self(support_input, support_attention)\n",
        "            loss = F.cross_entropy(logits, support_target)\n",
        "            loss.backward()\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.data -= lr_inner * param.grad\n",
        "            optimizer.zero_grad()  # Reset optimizer after inner-loop update\n",
        "\n",
        "        # Query step\n",
        "        query_input, query_attention, query_target = query_data\n",
        "        query_logits = self(query_input, query_attention)\n",
        "        query_loss = F.cross_entropy(query_logits, query_target)\n",
        "\n",
        "        # Restore original parameters\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.data = original_params[name]\n",
        "\n",
        "        return query_loss\n",
        "\n",
        "# Example usage of MAML Model\n",
        "if __name__ == \"__main__\":\n",
        "    model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "    maml_model = MAMLModel(model).to(device)\n",
        "    optimizer = AdamW(maml_model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Assuming `support_data` and `query_data` are DataLoader objects\n",
        "    for epoch in range(3):\n",
        "        for support_batch, query_batch in zip(train_dataloader, val_dataloader):\n",
        "            support_input, support_attention, support_target = support_batch\n",
        "            query_input, query_attention, query_target = query_batch\n",
        "\n",
        "            support_input, support_attention, support_target = support_input.to(device), support_attention.to(device), support_target.to(device)\n",
        "            query_input, query_attention, query_target = query_input.to(device), query_attention.to(device), query_target.to(device)\n",
        "\n",
        "            support_data = (support_input, support_attention, support_target)\n",
        "            query_data = (query_input, query_attention, query_target)\n",
        "\n",
        "            query_loss = maml_model.fast_adapt(support_data, query_data, optimizer)\n",
        "            print(f\"Epoch {epoch + 1}, Query Loss: {query_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi"
      ],
      "metadata": {
        "id": "m0rfrlIB7e-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI, Request\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=2048, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = task_data\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Load dataset dynamically from CSV\n",
        "def load_dataset(file_path, tokenizer, max_length=2048, for_classification=False):\n",
        "    df = pd.read_csv(file_path)\n",
        "    dataset = []\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"text\": row[\"text\"],\n",
        "            \"label\": row[\"label\"] if for_classification else None\n",
        "        }\n",
        "        dataset.append(data)\n",
        "    return TextDataset(dataset, tokenizer, max_length=max_length, for_classification=for_classification)\n",
        "\n",
        "# Optuna for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-4)\n",
        "    adapter_dim = trial.suggest_int(\"adapter_dim\", 16, 128, step=16)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
        "\n",
        "    # Create model and dataset\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks={\"classification\": 3}, adapter_dim=adapter_dim).to(device)\n",
        "    train_data, val_data = load_dataset(\"train.csv\", tokenizer, for_classification=True), load_dataset(\"val.csv\", tokenizer, for_classification=True)\n",
        "\n",
        "    # Train\n",
        "    train_with_scheduler(model, {\"classification\": train_data}, epochs=3, batch_size=batch_size, learning_rate=learning_rate, accumulation_steps=2, num_training_steps=1000)\n",
        "\n",
        "    # Evaluate\n",
        "    precision, recall, f1 = evaluate_with_metrics(model, val_data, \"classification\")\n",
        "    return f1  # Optimize F1 score\n",
        "\n",
        "# Run Optuna study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best trial:\", study.best_trial)\n",
        "\n",
        "# FastAPI deployment\n",
        "app = FastAPI()\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model.eval()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Request):\n",
        "    data = await request.json()\n",
        "    text = data[\"text\"]\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=2048, return_tensors=\"pt\").to(device)\n",
        "    input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
        "\n",
        "    # Predict\n",
        "    _, logits = model(input_ids, attention_mask, task=\"classification\")\n",
        "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    return {\"predictions\": predictions.tolist()}\n",
        "\n",
        "# To run the FastAPI app, use:\n",
        "# uvicorn script_name:app"
      ],
      "metadata": {
        "id": "ttdo-eaeb7Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI, Request\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=2048, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = task_data\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Load dataset dynamically from CSV\n",
        "def load_dataset(file_path, tokenizer, max_length=2048, for_classification=False):\n",
        "    df = pd.read_csv(file_path)\n",
        "    dataset = []\n",
        "    for _, row in df.iterrows():\n",
        "        data = {\n",
        "            \"text\": row[\"text\"],\n",
        "            \"label\": row[\"label\"] if for_classification else None\n",
        "        }\n",
        "        dataset.append(data)\n",
        "    return TextDataset(dataset, tokenizer, max_length=max_length, for_classification=for_classification)\n",
        "\n",
        "# Optuna for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-4)\n",
        "    adapter_dim = trial.suggest_int(\"adapter_dim\", 16, 128, step=16)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
        "\n",
        "    # Create model and dataset\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks={\"classification\": 3}, adapter_dim=adapter_dim).to(device)\n",
        "    train_data, val_data = load_dataset(\"train.csv\", tokenizer, for_classification=True), load_dataset(\"val.csv\", tokenizer, for_classification=True)\n",
        "\n",
        "    # Train\n",
        "    train_with_scheduler(model, {\"classification\": train_data}, epochs=3, batch_size=batch_size, learning_rate=learning_rate, accumulation_steps=2, num_training_steps=1000)\n",
        "\n",
        "    # Evaluate\n",
        "    precision, recall, f1 = evaluate_with_metrics(model, val_data, \"classification\")\n",
        "    return f1  # Optimize F1 score\n",
        "\n",
        "# Run Optuna study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best trial:\", study.best_trial)\n",
        "\n",
        "# FastAPI deployment\n",
        "app = FastAPI()\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model.eval()\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Request):\n",
        "    data = await request.json()\n",
        "    text = data[\"text\"]\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=2048, return_tensors=\"pt\").to(device)\n",
        "    input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
        "\n",
        "    # Predict\n",
        "    _, logits = model(input_ids, attention_mask, task=\"classification\")\n",
        "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    return {\"predictions\": predictions.tolist()}\n",
        "\n",
        "# To run the FastAPI app, use:\n",
        "# uvicorn script_name:app --reload"
      ],
      "metadata": {
        "id": "OISZJMOx-pOc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}