{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOu7SnlRPux42AY6675mKlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/LongformerClassificationModel(nn_Module).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUrdNkRrcbwK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import LongformerModel\n",
        "\n",
        "class LongformerClassificationModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(LongformerClassificationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, 2)  # Adjust the output size as per your requirement\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids, attention_mask)\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Use the [CLS] token representation for classification\n",
        "        return logits\n",
        "\n",
        "    def clone_parameters(self):\n",
        "        return {name: param.clone() for name, param in self.named_parameters()}\n",
        "\n",
        "    def load_parameters(self, params):\n",
        "        for name, param in self.named_parameters():\n",
        "            param.data.copy_(params[name].data)\n",
        "\n",
        "    def fast_adapt(self, support_data, query_data, optimizer, n_steps=5, lr_inner=1e-3):\n",
        "        support_input, support_attention, support_target = support_data\n",
        "        query_input, query_attention, query_target = query_data\n",
        "\n",
        "        original_params = self.clone_parameters()\n",
        "        for _ in range(n_steps):\n",
        "            optimizer.zero_grad()\n",
        "            logits = self(support_input, support_attention)\n",
        "            loss = F.cross_entropy(logits, support_target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        query_loss = F.cross_entropy(self(query_input, query_attention), query_target)\n",
        "        self.load_parameters(original_params)\n",
        "        return query_loss\n",
        "\n",
        "# Example usage\n",
        "model_name = \"allenai/longformer-base-4096\"\n",
        "model = LongformerClassificationModel(model_name)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Dummy data for example\n",
        "support_input = torch.randint(0, 100, (8, 512))\n",
        "support_attention = torch.ones_like(support_input)\n",
        "support_target = torch.randint(0, 2, (8,))\n",
        "\n",
        "query_input = torch.randint(0, 100, (8, 512))\n",
        "query_attention = torch.ones_like(query_input)\n",
        "query_target = torch.randint(0, 2, (8,))\n",
        "\n",
        "support_data = (support_input, support_attention, support_target)\n",
        "query_data = (query_input, query_attention, query_target)\n",
        "\n",
        "query_loss = model.fast_adapt(support_data, query_data, optimizer)\n",
        "print(f\"Query Loss: {query_loss.item()}\")"
      ]
    }
  ]
}