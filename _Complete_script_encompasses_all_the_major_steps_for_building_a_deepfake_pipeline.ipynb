{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO2mf1Q+w5B6kolZYa/8+Qn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/_Complete_script_encompasses_all_the_major_steps_for_building_a_deepfake_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sO7VKNoQzJz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import dnnlib\n",
        "import legacy\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ==========================\n",
        "# Load Pre-trained StyleGAN2 Model\n",
        "# ==========================\n",
        "def load_stylegan_model(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return legacy.load_network_pkl(f)['G_ema'].to('cuda')\n",
        "\n",
        "def generate_image(G):\n",
        "    z = torch.randn([1, G.z_dim]).cuda()  # Random latent vector\n",
        "    img = G(z, None)  # Generate image with no conditioning\n",
        "    img = (img.permute(0, 2, 3, 1) * 127.5 + 127.5).clamp(0, 255).to(torch.uint8)\n",
        "    return img\n",
        "\n",
        "def display_image(img):\n",
        "    plt.imshow(img[0].cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# ==========================\n",
        "# Define Autoencoder Model\n",
        "# ==========================\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc = nn.Linear(256 * 8 * 8, 1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(-1, 256 * 8 * 8)\n",
        "        return torch.relu(self.fc(x))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(1024, 256 * 8 * 8)\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc(x)).view(-1, 256, 8, 8)\n",
        "        x = torch.relu(self.deconv1(x))\n",
        "        x = torch.relu(self.deconv2(x))\n",
        "        return torch.sigmoid(self.deconv3(x))\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "# ==========================\n",
        "# Training Setup and Loop\n",
        "# ==========================\n",
        "def train_autoencoder(model_path='deepfake_autoencoder.pth', epochs=50):\n",
        "    # Hyperparameters\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Data preparation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.ImageFolder('/path/to/faces/dataset', transform=transform)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Model and optimizer setup\n",
        "    model = Autoencoder().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass and loss calculation\n",
        "            reconstructed = model(images)\n",
        "            loss = criterion(reconstructed, images)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Save the model state\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# ==========================\n",
        "# Wav2Lip Inference Script Execution\n",
        "# ==========================\n",
        "def run_wav2lip(audio_path: str, video_path: str, output_path: str,\n",
        "                checkpoint_path: str):\n",
        "    subprocess.run([\n",
        "        \"python\", \"inference.py\",\n",
        "        \"--checkpoint_path\", checkpoint_path,\n",
        "        \"--face\", video_path,\n",
        "        \"--audio\", audio_path,\n",
        "        \"--outfile\", output_path\n",
        "    ])\n",
        "\n",
        "# ==========================\n",
        "# Face Alignment Functionality\n",
        "# ==========================\n",
        "def align_face(image):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(\"/path/to/shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "    faces = detector(image)\n",
        "\n",
        "    for face in faces:\n",
        "        landmarks = predictor(image, face)\n",
        "\n",
        "        left_eye = (landmarks.part(36).x, landmarks.part(36).y)\n",
        "        right_eye = (landmarks.part(45).x, landmarks.part(45).y)\n",
        "\n",
        "        eye_center = ((left_eye[0] + right_eye[0]) // 2,\n",
        "                       (left_eye[1] + right_eye[1]) // 2)\n",
        "\n",
        "        delta_x = right_eye[0] - left_eye[0]\n",
        "        delta_y = right_eye[1] - left_eye[1]\n",
        "\n",
        "        angle = np.arctan2(delta_y, delta_x) * 180 / np.pi\n",
        "\n",
        "        desired_face_width = 256\n",
        "        desired_face_height = 256\n",
        "\n",
        "        M = cv2.getRotationMatrix2D(eye_center, angle, 1.0)\n",
        "\n",
        "        t_x = desired_face_width * 0.5\n",
        "        t_y = desired_face_height * 0.35\n",
        "\n",
        "        M[0, 2] += (t_x - eye_center[0])\n",
        "        M[1, 2] += (t_y - eye_center[1])\n",
        "\n",
        "        aligned_face = cv2.warpAffine(image, M,\n",
        "                                       (desired_face_width,\n",
        "                                        desired_face_height))\n",
        "\n",
        "        return aligned_face\n",
        "\n",
        "    return None\n",
        "\n",
        "# ==========================\n",
        "# Seamless Cloning Functionality\n",
        "# ==========================\n",
        "def seamless_clone(generated_face_path: str,\n",
        "                   target_image_path: str,\n",
        "                   output_image_path: str):\n",
        "\n",
        "    generated_face = cv2.imread(generated_face_path)\n",
        "    target_image = cv2.imread(target_image_path)\n",
        "\n",
        "    if generated_face is None or target_image is None:\n",
        "        print(\"Error loading images.\")\n",
        "        return\n",
        "\n",
        "    mask = np.zeros_like(target_image[:, :, 0])\n",
        "\n",
        "    face_position = slice(100, 300), slice(100, 300)\n",
        "    mask[face_position] = 255\n",
        "\n",
        "    center_position = (200, 200)\n",
        "\n",
        "    blended_image = cv2.seamlessClone(\n",
        "        src=generated_face,\n",
        "        dst=target_image,\n",
        "        mask=mask,\n",
        "        p=center_position,\n",
        "        flags=cv2.NORMAL_CLONE\n",
        "    )\n",
        "\n",
        "    cv2.imwrite(output_image_path, blended_image)\n",
        "\n",
        "# ==========================\n",
        "# Video Processing Functionality\n",
        "# ==========================\n",
        "def process_output_video(input_video_path: str,\n",
        "                         output_video_path: str):\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    out = cv2.VideoWriter(output_video_path,\n",
        "                          cv2.VideoWriter_fourcc(*'MP4V'),\n",
        "                          fps,\n",
        "                          (frame_width,\n",
        "                           frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            processed_frame = cv2.GaussianBlur(frame,(5 ,5),0)\n",
        "            out.write(processed_frame)\n",
        "\n",
        "            continue\n",
        "\n",
        "        break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()"
      ]
    }
  ]
}