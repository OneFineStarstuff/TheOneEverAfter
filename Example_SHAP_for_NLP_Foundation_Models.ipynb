{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMp5lOWfToIl880DlsiG83R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Example_SHAP_for_NLP_Foundation_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIFI04NDKFpl"
      },
      "outputs": [],
      "source": [
        "pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Dummy input text data (Ensure this is a list of strings)\n",
        "text_data = [\"This is a sample text for model explanation.\"]\n",
        "\n",
        "# Tokenize the text data for use in prediction\n",
        "inputs = tokenizer(text_data, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Define a wrapper for the model to use with SHAP\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWrapper, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.logits\n",
        "\n",
        "model_wrapper = ModelWrapper(model)\n",
        "\n",
        "# Define the prediction function\n",
        "def predict(input_texts):\n",
        "    # Check if input_texts is a numpy ndarray, if so, convert to list of strings\n",
        "    if isinstance(input_texts, np.ndarray):\n",
        "        input_texts = input_texts.tolist()\n",
        "\n",
        "    # Ensure input_texts is a list of strings\n",
        "    if isinstance(input_texts, list) and all(isinstance(i, str) for i in input_texts):\n",
        "        # Tokenize input (support for batch processing)\n",
        "        tokenized_inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            logits = model_wrapper(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])\n",
        "            return logits.cpu().numpy()\n",
        "    else:\n",
        "        raise ValueError(\"Input must be a list of strings.\")\n",
        "\n",
        "# Initialize the SHAP Explainer with a masker\n",
        "masker = shap.maskers.Text(tokenizer)\n",
        "\n",
        "# Initialize the SHAP Explainer\n",
        "explainer = shap.Explainer(predict, masker=masker)\n",
        "\n",
        "# Compute SHAP values\n",
        "shap_values = explainer(text_data)\n",
        "\n",
        "# Visualize the SHAP values\n",
        "shap.plots.text(shap_values[0])"
      ],
      "metadata": {
        "id": "irp1cIu62KC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "fvhcF0Uz327D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import shap\n",
        "\n",
        "# 1. Load Dataset (Here, we use IMDB for binary sentiment classification)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 2. Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# 3. Tokenize the dataset (this will handle padding and truncation)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Remove raw text for efficiency\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# 4. Split the dataset into train and validation\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "val_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "# 5. Initialize the model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # Adjust num_labels for multi-class tasks\n",
        "\n",
        "# 6. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # Output directory\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate at each epoch\n",
        "    learning_rate=2e-5,                  # Learning rate\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    weight_decay=0.01,                   # Weight decay\n",
        ")\n",
        "\n",
        "# 7. Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The model to train\n",
        "    args=training_args,                  # Training arguments\n",
        "    train_dataset=train_dataset,         # Training dataset\n",
        "    eval_dataset=val_dataset,            # Validation dataset\n",
        ")\n",
        "\n",
        "# 8. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 9. Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "# 10. Define a prediction function for SHAP\n",
        "def predict(input_texts):\n",
        "    inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    return logits.numpy()\n",
        "\n",
        "# 11. Initialize SHAP explainer\n",
        "explainer = shap.Explainer(predict, tokenizer)\n",
        "\n",
        "# 12. Example input text for SHAP explanation\n",
        "input_texts = [\"This is a sample text for model explanation.\"]\n",
        "\n",
        "# 13. Get SHAP values\n",
        "shap_values = explainer(input_texts)\n",
        "\n",
        "# 14. Visualize SHAP values for the first input text\n",
        "shap.plots.text(shap_values[0])"
      ],
      "metadata": {
        "id": "sxO8jloj3ux-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_texts):\n",
        "    print(f\"Input format: {type(input_texts)}\")  # Print input type\n",
        "    if isinstance(input_texts, list) and all(isinstance(i, str) for i in input_texts):\n",
        "        # Tokenize input (support for batch processing)\n",
        "        tokenized_inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            logits = model_wrapper(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])\n",
        "            return logits.cpu().numpy()\n",
        "    else:\n",
        "        raise ValueError(\"Input must be a list of strings.\")  # Raise an error if input format is incorrect"
      ],
      "metadata": {
        "id": "-dMgK_Y5zYY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}