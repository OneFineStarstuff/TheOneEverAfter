{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMuzT0OcDf4OAtAHNhTsntR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Full_training_pipeline_with_multitask_learning%2C_pruning%2C_and_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "blIxiMBAHunx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# MultiTask Adapter with adapters for task-specific tuning\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Distributed DataParallel initialization\n",
        "def init_ddp(rank, world_size, model):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    if not dist.is_initialized():\n",
        "        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "    ddp_model = DDP(model, find_unused_parameters=True)\n",
        "    return ddp_model\n",
        "\n",
        "# Train model with adapters, logging, and scheduler\n",
        "def train_with_scheduler(model, train_data, epochs, batch_size, learning_rate, log_dir=None,\n",
        "                         num_warmup_steps=0, num_training_steps=1000, rank=0):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir) if rank == 0 and log_dir else None\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for task, task_data in train_data.items():\n",
        "            train_dataset = TextDataset(task_data, model.tokenizer, for_classification=True)\n",
        "            sampler = DistributedSampler(train_dataset, num_replicas=1, rank=rank)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=sampler)\n",
        "\n",
        "            for batch in train_dataloader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    loss, logits = model(input_ids, attention_mask, task, labels=labels)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        if rank == 0 and log_dir:\n",
        "            writer.add_scalar(\"Loss/epoch\", total_loss / len(train_dataloader), epoch)\n",
        "\n",
        "    if writer:\n",
        "        writer.close()\n",
        "\n",
        "# Learning rate scheduler\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Evaluation with metrics\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                _, logits = model(input_ids, attention_mask, task)\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Example of usage\n",
        "nltk.download('wordnet')\n",
        "\n",
        "train_data = {\n",
        "    \"task1\": [{\"text\": \"example sentence for task 1\", \"label\": 0}],\n",
        "    \"task2\": [{\"text\": \"example sentence for task 2\", \"label\": 1}]\n",
        "}\n",
        "tasks = {\"task1\": 2, \"task2\": 2}\n",
        "model = MultiTaskAdapterFoundationModel(\"allenai/longformer-base-4096\", tasks).to(device)\n",
        "\n",
        "train_with_scheduler(model, train_data, epochs=3, batch_size=16, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "test_data = [{\"text\": \"example test sentence for task 1\", \"label\": 0}]\n",
        "test_dataset = TextDataset(test_data, model.tokenizer, for_classification=True)\n",
        "\n",
        "evaluate_with_metrics(model, test_dataset, \"task1\")"
      ],
      "metadata": {
        "id": "oUMlSVq5JYv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=2048, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = TextDataset(task_data, tokenizer, for_classification=True)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Example tasks and training data\n",
        "    tasks = {\"classification\": 3, \"sentiment\": 2}\n",
        "    train_data = {\n",
        "        \"classification\": [{\"text\": \"Sample input for classification\", \"label\": 0}],\n",
        "        \"sentiment\": [{\"text\": \"Sample input for sentiment analysis\", \"label\": 1}]\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks=tasks)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Train model\n",
        "    train_with_scheduler(model, train_data, epochs=3, batch_size=4, accumulation_steps=8, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "    # Test data\n",
        "    test_data = {\n",
        "        \"classification\": TextDataset([{\"text\": \"Test input\", \"label\": 0}], model.tokenizer, for_classification=True),\n",
        "        \"sentiment\": TextDataset([{\"text\": \"Another test input\", \"label\": 1}], model.tokenizer, for_classification=True)\n",
        "    }\n",
        "\n",
        "    # Evaluate model\n",
        "    for task, dataset in test_data.items():\n",
        "        print(f\"Evaluating task: {task}\")\n",
        "        evaluate_with_metrics(model, dataset, task)"
      ],
      "metadata": {
        "id": "sJjMLQlncSNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=2048, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item[\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = item[\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = task_data\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Augmenting the dataset with more examples and synonym replacement\n",
        "texts = [\n",
        "    {\"text\": \"The quick brown fox jumps over the lazy dog.\", \"label\": 0},\n",
        "    {\"text\": \"A journey of a thousand miles begins with a single step.\", \"label\": 0},\n",
        "    {\"text\": \"To be or not to be, that is the question.\", \"label\": 0},\n",
        "    {\"text\": \"All that glitters is not gold.\", \"label\": 0},\n",
        "    {\"text\": \"The early bird catches the worm.\", \"label\": 1},\n",
        "    {\"text\": \"A picture is worth a thousand words.\", \"label\": 1},\n",
        "    {\"text\": \"Better late than never.\", \"label\": 1},\n",
        "    {\"text\": \"Actions speak louder than words.\", \"label\": 1}\n",
        "]\n",
        "\n",
        "# Augmenting data with synonyms\n",
        "augmented_texts = []\n",
        "for text in texts:\n",
        "    for _ in range(3):  # Create 3 augmented versions of each sentence\n",
        "        augmented_text = synonym_replacement(text[\"text\"])\n",
        "        augmented_texts.append({\"text\": augmented_text, \"label\": text[\"label\"]})\n",
        "texts.extend(augmented_texts)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(texts)\n",
        "\n",
        "# Split data into training and validation sets again\n",
        "train_data, val_data = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data, tokenizer, for_classification=True)\n",
        "val_dataset = TextDataset(val_data, tokenizer, for_classification=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Train model again with the augmented dataset\n",
        "model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks={\"classification\": 3, \"sentiment\": 2}).to(device)\n",
        "train_with_scheduler(model, {\"classification\": train_dataset, \"sentiment\": train_dataset}, epochs=5, batch_size=4, accumulation_steps=8, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "# Evaluate model\n",
        "for task, dataset in {\"classification\": val_dataset, \"sentiment\": val_dataset}.items():\n",
        "    print(f\"Evaluating task: {task}\")\n",
        "    evaluate_with_metrics(model, dataset, task)"
      ],
      "metadata": {
        "id": "iXvqAM0JiaKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=8, accumulation_steps=4, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = TextDataset(task_data, tokenizer, for_classification=True)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Example tasks and training data\n",
        "    tasks = {\"classification\": 3, \"sentiment\": 2}\n",
        "    train_data = {\n",
        "        \"classification\": [{\"text\": \"Sample input for classification\", \"label\": 0}],\n",
        "        \"sentiment\": [{\"text\": \"Sample input for sentiment analysis\", \"label\": 1}]\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks=tasks)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Train model\n",
        "    train_with_scheduler(model, train_data, epochs=3, batch_size=8, accumulation_steps=4, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "    # Test data\n",
        "    test_data = {\n",
        "        \"classification\": TextDataset([{\"text\": \"Test input\", \"label\": 0}], model.tokenizer, for_classification=True),\n",
        "        \"sentiment\": TextDataset([{\"text\": \"Another test input\", \"label\": 1}], model.tokenizer, for_classification=True)\n",
        "    }\n",
        "\n",
        "    # Evaluate model\n",
        "    for task, dataset in test_data.items():\n",
        "        print(f\"Evaluating task: {task}\")\n",
        "        evaluate_with_metrics(model, dataset, task)"
      ],
      "metadata": {
        "id": "Vrld5F-cYaWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model with gradient accumulation\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=16, accumulation_steps=2, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = TextDataset(task_data, tokenizer, for_classification=True)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "                loss = loss / accumulation_steps\n",
        "                loss.backward()\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement for data augmentation\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Ensure wordnet is downloaded\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Example tasks and training data\n",
        "    tasks = {\"classification\": 3, \"sentiment\": 2}\n",
        "    train_data = {\n",
        "        \"classification\": [{\"text\": \"Sample input for classification\", \"label\": 0}],\n",
        "        \"sentiment\": [{\"text\": \"Sample input for sentiment analysis\", \"label\": 1}]\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks=tasks)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Train model\n",
        "    train_with_scheduler(model, train_data, epochs=3, batch_size=16, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "    # Test data\n",
        "    test_data = {\n",
        "        \"classification\": TextDataset([{\"text\": \"Test input\", \"label\": 0}], model.tokenizer, for_classification=True),\n",
        "        \"sentiment\": TextDataset([{\"text\": \"Another test input\", \"label\": 1}], model.tokenizer, for_classification=True)\n",
        "    }\n",
        "\n",
        "    # Evaluate model\n",
        "    for task, dataset in test_data.items():\n",
        "        print(f\"Evaluating task: {task}\")\n",
        "        evaluate_with_metrics(model, dataset, task)"
      ],
      "metadata": {
        "id": "ePfPZM-sU_GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=32, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for task, task_data in train_data.items():\n",
        "            tokenizer = model.tokenizer\n",
        "            train_dataset = TextDataset(task_data, tokenizer, for_classification=True)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                loss, _ = model(input_ids, attention_mask, task, labels=labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "        scheduler.step()\n",
        "        writer.add_scalar(\"Loss/train\", total_loss / len(train_dataloader), epoch)\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_dataloader)}\")\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Example tasks and training data\n",
        "    tasks = {\"classification\": 3, \"sentiment\": 2}\n",
        "    train_data = {\n",
        "        \"classification\": [{\"text\": \"Sample input for classification\", \"label\": 0}],\n",
        "        \"sentiment\": [{\"text\": \"Sample input for sentiment analysis\", \"label\": 1}]\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks=tasks)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Train model\n",
        "    train_with_scheduler(model, train_data, epochs=3, batch_size=16, learning_rate=5e-5, num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "    # Test data\n",
        "    test_data = {\n",
        "        \"classification\": TextDataset([{\"text\": \"Test input\", \"label\": 0}], model.tokenizer, for_classification=True),\n",
        "        \"sentiment\": TextDataset([{\"text\": \"Another test input\", \"label\": 1}], model.tokenizer, for_classification=True)\n",
        "    }\n",
        "\n",
        "    # Evaluate model\n",
        "    for task, dataset in test_data.items():\n",
        "        print(f\"Evaluating task: {task}\")\n",
        "        evaluate_with_metrics(model, dataset, task)"
      ],
      "metadata": {
        "id": "m8u44iSvQXxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmWJcDnJGfSY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import LongformerModel, LongformerTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.utils.prune as prune\n",
        "import optuna\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cpu\")  # Change to \"cuda\" if GPU is available\n",
        "\n",
        "# Define a custom dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=4096, for_classification=False):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.for_classification = for_classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        if self.for_classification:\n",
        "            label = self.data[idx][\"label\"]\n",
        "            return input_ids, attention_mask, label\n",
        "        else:\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "# Define LongformerFoundationModel class\n",
        "class LongformerFoundationModel(nn.Module):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\"):\n",
        "        super(LongformerFoundationModel, self).__init__()\n",
        "        self.model = LongformerModel.from_pretrained(model_name)\n",
        "        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        # Use sliding window attention for long sequences\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        return outputs\n",
        "\n",
        "# Define Adapter class\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim=64):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.down_proj = nn.Linear(input_dim, adapter_dim)\n",
        "        self.up_proj = nn.Linear(adapter_dim, input_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.dropout(self.up_proj(self.activation(self.down_proj(x))))\n",
        "\n",
        "# Define MultiTaskAdapterFoundationModel class\n",
        "class MultiTaskAdapterFoundationModel(LongformerFoundationModel):\n",
        "    def __init__(self, model_name=\"allenai/longformer-base-4096\", tasks=None, adapter_dim=64):\n",
        "        super().__init__(model_name)\n",
        "        self.tasks = tasks or {}\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            task: nn.Linear(self.model.config.hidden_size, num_labels) for task, num_labels in self.tasks.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask, global_attention_mask=(attention_mask == 1))\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        logits = self.classifiers[task](hidden_states[:, 0, :])  # CLS token\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifiers[task].out_features), labels.view(-1))\n",
        "        return loss, logits\n",
        "\n",
        "# Initialize DDP\n",
        "def init_ddp(rank, world_size, model):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    if not dist.is_initialized():\n",
        "        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "    ddp_model = DDP(model, find_unused_parameters=True)\n",
        "    return ddp_model\n",
        "\n",
        "# Scheduler for learning rate\n",
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Train the multitask model\n",
        "def train_with_scheduler(model, train_data, epochs=5, batch_size=32, learning_rate=5e-5, log_dir=\"./logs\", num_warmup_steps=500, num_training_steps=10000, rank=None):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps, num_training_steps)\n",
        "    writer = SummaryWriter(log_dir=log_dir) if rank == 0 else None\n",
        "    scaler = GradScaler()\n",
        "    checkpoint_dir = \"./checkpoints\"\n",
        "    if not os.path.exists(checkpoint_dir) and rank == 0:\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for task, task_data in train_data.items():\n",
        "            # Fix for tokenizer access\n",
        "            tokenizer = model.module.tokenizer if hasattr(model, \"module\") else model.tokenizer\n",
        "            train_dataset = TextDataset(task_data, tokenizer, for_classification=True)\n",
        "            sampler = DistributedSampler(train_dataset, num_replicas=1, rank=rank)\n",
        "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=sampler)\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                with autocast():\n",
        "                    loss, logits = model(input_ids, attention_mask, task, labels=labels)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                total_loss += loss.item()\n",
        "                if rank == 0:\n",
        "                    writer.add_scalar(f\"Loss/train_{task}\", loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
        "        if rank == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Task: {task}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "            torch.save(model.state_dict(), f\"./checkpoints/model_epoch_{epoch+1}.pt\")\n",
        "    if rank == 0:\n",
        "        writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_with_metrics(model, test_data, task, batch_size=32):\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            with autocast():\n",
        "                _, logits = model(input_ids, attention_mask, task)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_preds, all_labels, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Synonym replacement\n",
        "def synonym_replacement(text, n=2):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random.shuffle(words)\n",
        "    num_replaced = 0\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if w == word and num_replaced < n else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-4)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64])\n",
        "    adapter_dim = trial.suggest_int('adapter_dim', 16, 128, step=16)\n",
        "\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\",\n",
        "                                            tasks={\"task1\": 2}, adapter_dim=adapter_dim)\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_data = {\"task1\": [{\"text\": \"Example sentence\", \"label\": 1}]}  # Replace with actual training data\n",
        "    train_with_scheduler(model, train_data, epochs=3, batch_size=batch_size,\n",
        "                         learning_rate=learning_rate, num_warmup_steps=100, num_training_steps=500)\n",
        "\n",
        "    test_data = TextDataset([{\"text\": \"Example sentence\", \"label\": 1}],\n",
        "                            model.tokenizer, for_classification=True)  # Replace with actual test data\n",
        "    precision, recall, f1 = evaluate_with_metrics(model, test_data, task=\"task1\", batch_size=batch_size)\n",
        "\n",
        "    return f1  # Maximize F1 score\n",
        "\n",
        "# Run Optuna study\n",
        "def run_optuna():\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print(f\"Best trial: {study.best_trial.params}\")\n",
        "\n",
        "# Apply pruning\n",
        "def apply_pruning(model, amount=0.3):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            prune.remove(module, \"weight\")  # Remove reparameterization after pruning\n",
        "    print(\"Pruning applied successfully!\")\n",
        "\n",
        "# Check sparsity\n",
        "def check_sparsity(model):\n",
        "    total_params = 0\n",
        "    zero_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            total_params += param.numel()\n",
        "            zero_params += (param == 0).sum().item()\n",
        "    sparsity = 100.0 * zero_params / total_params\n",
        "    print(f\"Model sparsity: {sparsity:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Example tasks and training data\n",
        "    tasks = {\"classification\": 3, \"sentiment\": 2}\n",
        "    train_data = {\n",
        "        \"classification\": [{\"text\": \"Sample input for classification\", \"label\": 0}],\n",
        "        \"sentiment\": [{\"text\": \"Sample input for sentiment analysis\", \"label\": 1}]\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiTaskAdapterFoundationModel(model_name=\"allenai/longformer-base-4096\", tasks=tasks)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Train model\n",
        "    train_with_scheduler(model, train_data, epochs=3, batch_size=16, learning_rate=5e-5,\n",
        "                         num_warmup_steps=100, num_training_steps=1000)\n",
        "\n",
        "    # Apply pruning\n",
        "    apply_pruning(model, amount=0.3)\n",
        "    check_sparsity(model)\n",
        "\n",
        "    # Evaluate\n",
        "    test_data = {\n",
        "        \"classification\": TextDataset([{\"text\": \"Test input\", \"label\": 0}], model.tokenizer, for_classification=True),\n",
        "        \"sentiment\": TextDataset([{\"text\": \"Another test input\", \"label\": 1}], model.tokenizer, for_classification=True)\n",
        "    }\n",
        "\n",
        "    for task, dataset in test_data.items():\n",
        "        print(f\"Evaluating task: {task}\")\n",
        "        evaluate_with_metrics(model, dataset, task)\n",
        "\n",
        "    # Run Optuna\n",
        "    run_optuna()"
      ]
    }
  ]
}