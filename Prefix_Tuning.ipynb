{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPPUnls5mVwtG7TkWs7KAO/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Prefix_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ37H3jOGz5Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class PrefixTuning(nn.Module):\n",
        "    def __init__(self, prefix_length=10, embedding_dim=768):\n",
        "        super(PrefixTuning, self).__init__()\n",
        "        self.prefix_tokens = nn.Parameter(torch.randn(1, prefix_length, embedding_dim))\n",
        "        self.embedding_layer = nn.Embedding(30522, embedding_dim)  # Assuming BERT vocabulary size\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        batch_size = input_ids.size(0)\n",
        "        prefix = self.prefix_tokens.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Get embeddings for input IDs\n",
        "        input_embeddings = self.embedding_layer(input_ids)\n",
        "\n",
        "        input_with_prefix = torch.cat((prefix, input_embeddings), dim=1)\n",
        "        prefix_attention_mask = torch.ones(batch_size, prefix.size(1), device=input_ids.device)\n",
        "        attention_with_prefix = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
        "        return input_with_prefix, attention_with_prefix\n",
        "\n",
        "# Example usage\n",
        "class PrefixTuningModel(nn.Module):\n",
        "    def __init__(self, model, prefix_tuning):\n",
        "        super(PrefixTuningModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.prefix_tuning = prefix_tuning\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        input_with_prefix, attention_with_prefix = self.prefix_tuning(input_ids, attention_mask)\n",
        "        outputs = self.model(inputs_embeds=input_with_prefix, attention_mask=attention_with_prefix)\n",
        "        return outputs\n",
        "\n",
        "# Load a pre-trained transformer model from Hugging Face\n",
        "base_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Initialize the PrefixTuning and PrefixTuningModel\n",
        "prefix_tuning = PrefixTuning(prefix_length=10, embedding_dim=768)\n",
        "prefix_model = PrefixTuningModel(base_model, prefix_tuning)\n",
        "\n",
        "# Create a dummy input\n",
        "input_text = \"This is an example sentence.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the PrefixTuningModel\n",
        "outputs = prefix_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "print(outputs)"
      ]
    }
  ]
}