{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOADETVvWSwz2e0Ds9XI+r8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/TheOneEverAfter/blob/main/_Quantum_Reinforcement_Learning_(QRL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pennylane"
      ],
      "metadata": {
        "id": "-JCDBk3DzCkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twehNpdFy0to"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "\n",
        "# Define a quantum device with 2 qubits\n",
        "dev = qml.device(\"default.qubit\", wires=2)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL\n",
        "@qml.qnode(dev)\n",
        "def quantum_policy(state):\n",
        "    qml.RX(state[0], wires=0)\n",
        "    qml.RY(state[1], wires=1)\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# Example state input\n",
        "state = [0.5, 1.0]\n",
        "\n",
        "# Execute the quantum circuit with the given state\n",
        "policy_output = quantum_policy(state)\n",
        "\n",
        "print(f'Policy Output: {policy_output}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a quantum device with 2 qubits\n",
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL\n",
        "n_layers = 3\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        for j in range(n_qubits):\n",
        "            qml.RX(weights[i, j, 0], wires=j)\n",
        "            qml.RY(weights[i, j, 1], wires=j)\n",
        "            qml.RZ(weights[i, j, 2], wires=j)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define the classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, 3))\n",
        "        self.classical_nn = ClassicalNN(4, n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Example state input\n",
        "state = torch.tensor([0.5, 1.0, -0.5, 0.3], requires_grad=True)  # Example: state of dimension 4\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop (simplified)\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    policy_output = qrl_policy(state)\n",
        "    loss = -torch.sum(policy_output)  # Example: Dummy loss function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy):\n",
        "        self.policy = policy\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, requires_grad=True)\n",
        "        action = self.policy(state).detach().numpy()\n",
        "        return np.argmax(action)  # Example: Select action with highest value\n",
        "\n",
        "# Instantiate the environment and test the policy\n",
        "env = SimpleQRL(qrl_policy)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "7-lxe97lOQbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a quantum device with 2 qubits\n",
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL\n",
        "n_layers = 3\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        for j in range(n_qubits):\n",
        "            qml.RX(weights[i, j, 0], wires=j)\n",
        "            qml.RY(weights[i, j, 1], wires=j)\n",
        "            qml.RZ(weights[i, j, 2], wires=j)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define the classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, 3))\n",
        "        self.classical_nn = ClassicalNN(4, n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Example state input\n",
        "state = torch.tensor([0.5, 1.0, -0.5, 0.3], requires_grad=True)  # Example: state of dimension 4\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop (with more complex loss function and scheduler)\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    policy_output = qrl_policy(state)\n",
        "    loss = -torch.mean(policy_output)  # More complex loss function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy):\n",
        "        self.policy = policy\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, requires_grad=True)\n",
        "        action = self.policy(state).detach().numpy()\n",
        "        return np.argmax(action)  # Example: Select action with highest value\n",
        "\n",
        "# Instantiate the environment and test the policy\n",
        "env = SimpleQRL(qrl_policy)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "BsxiuQlcPE-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILNgLm0SSs-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a quantum device with 2 qubits\n",
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define the classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(4, n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Example state input\n",
        "state = torch.tensor([0.5, 1.0, -0.5, 0.3], requires_grad=True)  # Example: state of dimension 4\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop with advanced loss function and scheduler\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    policy_output = qrl_policy(state)\n",
        "    loss = -torch.mean(policy_output)  # Example: More sophisticated loss function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy):\n",
        "        self.policy = policy\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, requires_grad=True)\n",
        "        action = self.policy(state).detach().numpy()\n",
        "        return np.argmax(action)  # Example: Select action with highest value\n",
        "\n",
        "# Instantiate the environment and test the policy\n",
        "env = SimpleQRL(qrl_policy)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "kE1_MFfqPnvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-CgWbrqSy0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a quantum device with more qubits\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more qubits and complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define a more complex classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)  # Ensure proper shape for LSTM input: (batch_size, seq_length, input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])  # Remove extra dimension\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Example state input\n",
        "state = torch.tensor([[0.5, 1.0, -0.5, 0.3]], dtype=torch.float32, requires_grad=True)  # Example: batched state of dimension 4\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop with advanced loss function and scheduler\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    policy_output = qrl_policy(state)\n",
        "    loss = -torch.mean(policy_output)  # Example: More sophisticated loss function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy):\n",
        "        self.policy = policy\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        action = self.policy(state).detach().numpy()\n",
        "        return np.argmax(action)  # Example: Select action with highest value\n",
        "\n",
        "# Instantiate the environment and test the policy\n",
        "env = SimpleQRL(qrl_policy)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "3vKI1TFXQrdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1T6MU9dvTuCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy, epsilon=0.1):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, len(state) - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.policy(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "# Instantiate the environment and test the policy with epsilon-greedy strategy\n",
        "env = SimpleQRL(qrl_policy, epsilon=0.1)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "_ooYzFmrTujb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Define a quantum device with more qubits\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more qubits and complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define a more complex classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)  # Ensure proper shape for LSTM input: (batch_size, seq_length, input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])  # Remove extra dimension\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Example state input\n",
        "state = torch.tensor([[0.5, 1.0, -0.5, 0.3]], dtype=torch.float32, requires_grad=True)  # Example: batched state of dimension 4\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop with advanced loss function and scheduler\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    policy_output = qrl_policy(state)\n",
        "    loss = -torch.mean(policy_output)  # Example: More sophisticated loss function\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy, epsilon=0.1):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, len(state) - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.policy(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "# Instantiate the environment and test the policy with epsilon-greedy strategy\n",
        "env = SimpleQRL(qrl_policy, epsilon=0.1)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "6fPOs-VpUQkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9kzwLnSYosQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Define a quantum device with more qubits\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more qubits and complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define a more complex classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)  # Ensure proper shape for LSTM input: (batch_size, seq_length, input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])  # Remove extra dimension\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop with REINFORCE\n",
        "n_epochs = 100\n",
        "gamma = 0.99  # Discount factor\n",
        "for epoch in range(n_epochs):\n",
        "    rewards = []\n",
        "    states = []\n",
        "    log_probs = []\n",
        "\n",
        "    for _ in range(10):  # Example: Collect 10 trajectories\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)  # Random state\n",
        "        policy_output = qrl_policy(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()  # Sample action based on probabilities\n",
        "        reward = random.uniform(-1, 1)  # Example: Random reward\n",
        "\n",
        "        states.append(state)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))  # Correct indexing for action_prob tensor\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Compute discounted rewards\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    # Normalize rewards\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    # Update policy\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = []\n",
        "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "    policy_loss = torch.stack(policy_loss).sum()  # Use torch.stack instead of torch.cat for proper stacking\n",
        "\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {policy_loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy, epsilon=0.1):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, state.shape[-1] - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.policy(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "# Instantiate the environment and test the policy with epsilon-greedy strategy\n",
        "env = SimpleQRL(qrl_policy, epsilon=0.1)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "action = env.get_action(test_state)\n",
        "print(f'Selected Action: {action}')"
      ],
      "metadata": {
        "id": "Uu2B-OThYqBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYn6YFzaaSsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleQRL:\n",
        "    def __init__(self, policy, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, state.shape[-1] - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.policy(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Instantiate the environment with dynamic epsilon strategy\n",
        "env = SimpleQRL(qrl_policy, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "for i in range(100):\n",
        "    action = env.get_action(test_state)\n",
        "    env.decay_epsilon()\n",
        "    print(f'Selected Action: {action}, Epsilon: {env.epsilon:.4f}')"
      ],
      "metadata": {
        "id": "yqUiHxUWaTSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Define a quantum device with more qubits\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more qubits and complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define a more complex classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)  # Ensure proper shape for LSTM input: (batch_size, seq_length, input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])  # Remove extra dimension\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop with REINFORCE\n",
        "n_epochs = 100\n",
        "gamma = 0.99  # Discount factor\n",
        "for epoch in range(n_epochs):\n",
        "    rewards = []\n",
        "    states = []\n",
        "    log_probs = []\n",
        "\n",
        "    for _ in range(10):  # Example: Collect 10 trajectories\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)  # Random state\n",
        "        policy_output = qrl_policy(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()  # Sample action based on probabilities\n",
        "        reward = random.uniform(-1, 1)  # Example: Random reward\n",
        "\n",
        "        states.append(state)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))  # Correct indexing for action_prob tensor\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Compute discounted rewards\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    # Normalize rewards\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    # Update policy\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = []\n",
        "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "    policy_loss = torch.stack(policy_loss).sum()  # Use torch.stack instead of torch.cat for proper stacking\n",
        "\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {policy_loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, state.shape[-1] - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.policy(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Instantiate the environment with dynamic epsilon strategy\n",
        "env = SimpleQRL(qrl_policy, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "for i in range(100):\n",
        "    action = env.get_action(test_state)\n",
        "    env.decay_epsilon()\n",
        "    print(f'Selected Action: {action}, Epsilon: {env.epsilon:.4f}')"
      ],
      "metadata": {
        "id": "usQNEN59avBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7z73UuMch8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Define a quantum device with more qubits\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more qubits and complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define a more complex classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)  # Ensure proper shape for LSTM input: (batch_size, seq_length, input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])  # Remove extra dimension\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Initialize the QRL policy and optimizer\n",
        "qrl_policy = QuantumPolicy(n_qubits, n_layers)\n",
        "optimizer = optim.Adam(qrl_policy.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training loop with REINFORCE\n",
        "n_epochs = 100\n",
        "gamma = 0.99  # Discount factor\n",
        "for epoch in range(n_epochs):\n",
        "    rewards = []\n",
        "    states = []\n",
        "    log_probs = []\n",
        "\n",
        "    for _ in range(10):  # Example: Collect 10 trajectories\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)  # Random state\n",
        "        policy_output = qrl_policy(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()  # Sample action based on probabilities\n",
        "        reward = random.uniform(-1, 1)  # Example: Random reward\n",
        "\n",
        "        states.append(state)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))  # Correct indexing for action_prob tensor\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Compute discounted rewards\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    # Normalize rewards\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    # Update policy\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = []\n",
        "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "    policy_loss = torch.stack(policy_loss).sum()  # Use torch.stack instead of torch.cat for proper stacking\n",
        "\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {policy_loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection\n",
        "class SimpleQRL:\n",
        "    def __init__(self, policy, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01):\n",
        "        self.policy = policy\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32, requires_grad=True).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, state.shape[-1] - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.policy(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Instantiate the environment with dynamic epsilon strategy\n",
        "env = SimpleQRL(qrl_policy, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "for i in range(100):\n",
        "    action = env.get_action(test_state)\n",
        "    env.decay_epsilon()\n",
        "    print(f'Selected Action: {action}, Epsilon: {env.epsilon:.4f}')"
      ],
      "metadata": {
        "id": "6ihC_-aacjTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AYMkOAeWed1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "y2daNk9GfXOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Actor-Critic networks and optimizer\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with Actor-Critic and refined reward functions\n",
        "n_epochs = 100\n",
        "gamma = 0.99  # Discount factor\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    rewards = []\n",
        "    log_probs = []\n",
        "    state_values = []\n",
        "\n",
        "    for _ in range(10):  # Example: Collect 10 trajectories\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)  # Random state\n",
        "        policy_output = actor(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()  # Sample action based on probabilities\n",
        "        reward = random.uniform(-1, 1)  # Example: Refined reward function\n",
        "\n",
        "        value = critic(state).squeeze()  # Squeeze to ensure correct dimensions\n",
        "        state_values.append(value)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Compute discounted rewards\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    # Normalize rewards\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    # Update policy and value networks\n",
        "    actor_optimizer.zero_grad()\n",
        "    critic_optimizer.zero_grad()\n",
        "    policy_loss = []\n",
        "    value_loss = []\n",
        "    for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "        advantage = reward - value.item()\n",
        "        policy_loss.append(-log_prob * advantage)\n",
        "        value_loss.append(nn.MSELoss()(value, torch.tensor([reward]).squeeze()))  # Squeeze target reward\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    value_loss = torch.stack(value_loss).sum()\n",
        "\n",
        "    policy_loss.backward()\n",
        "    value_loss.backward()\n",
        "\n",
        "    actor_optimizer.step()\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Actor Loss: {policy_loss.item():.4f}, Critic Loss: {value_loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection with dynamic epsilon strategy\n",
        "class SimpleQRL:\n",
        "    def __init__(self, actor, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01):\n",
        "        self.actor = actor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, state.shape[-1] - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.actor(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Instantiate the environment with dynamic epsilon strategy\n",
        "env = SimpleQRL(actor, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "for i in range(100):\n",
        "    action = env.get_action(test_state)\n",
        "    env.decay_epsilon()\n",
        "    print(f'Selected Action: {action}, Epsilon: {env.epsilon:.4f}')"
      ],
      "metadata": {
        "id": "ZSbIw7QAg_ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "import random\n",
        "\n",
        "# Define a quantum device with more qubits\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "# Quantum circuit used as the policy in QRL with more qubits and complex architecture\n",
        "n_layers = 4\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def quantum_policy(state, weights):\n",
        "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
        "    for i in range(n_layers):\n",
        "        qml.BasicEntanglerLayers(weights[i], wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "# Define a more complex classical neural network\n",
        "class ClassicalNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ClassicalNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)  # Ensure proper shape for LSTM input: (batch_size, seq_length, input_size)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])  # Remove extra dimension\n",
        "        return x\n",
        "\n",
        "# Define the QRL policy\n",
        "class QuantumPolicy(nn.Module):\n",
        "    def __init__(self, n_qubits, n_layers):\n",
        "        super(QuantumPolicy, self).__init__()\n",
        "        self.weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, n_qubits))\n",
        "        self.classical_nn = ClassicalNN(input_dim=4, hidden_dim=128, output_dim=n_qubits)  # Example: Input state dimension 4\n",
        "\n",
        "    def forward(self, x):\n",
        "        classical_output = self.classical_nn(x)\n",
        "        quantum_input = classical_output.detach().numpy()\n",
        "        quantum_output = quantum_policy(quantum_input, self.weights)\n",
        "        return torch.tensor(quantum_output, requires_grad=True)\n",
        "\n",
        "# Define the Critic network\n",
        "class CriticNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(CriticNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc2(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# Initialize the Actor-Critic networks and optimizer\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "# Hyperparameter tuning with Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    actor_lr = trial.suggest_float('actor_lr', 1e-5, 1e-2, log=True)\n",
        "    critic_lr = trial.suggest_float('critic_lr', 1e-5, 1e-2, log=True)\n",
        "\n",
        "    # Initialize optimizers with suggested learning rates\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "\n",
        "    # Training loop with Actor-Critic and refined reward functions\n",
        "    n_epochs = 100\n",
        "    gamma = 0.99  # Discount factor\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        state_values = []\n",
        "\n",
        "        for _ in range(10):  # Example: Collect 10 trajectories\n",
        "            state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)  # Random state\n",
        "            policy_output = actor(state)\n",
        "            action_prob = torch.softmax(policy_output, dim=-1)\n",
        "            action = torch.multinomial(action_prob, num_samples=1).item()  # Sample action based on probabilities\n",
        "            reward = random.uniform(-1, 1)  # Example: Refined reward function\n",
        "\n",
        "            value = critic(state)\n",
        "            state_values.append(value)\n",
        "            log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "            rewards.append(reward)\n",
        "\n",
        "        # Compute discounted rewards\n",
        "        discounted_rewards = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            discounted_rewards.insert(0, R)\n",
        "\n",
        "        # Normalize rewards\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "        # Update policy and value networks\n",
        "        actor_optimizer.zero_grad()\n",
        "        critic_optimizer.zero_grad()\n",
        "        policy_loss = []\n",
        "        value_loss = []\n",
        "        for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "            advantage = reward - value.item()\n",
        "            policy_loss.append(-log_prob * advantage)\n",
        "            value_loss.append(nn.MSELoss()(value, torch.tensor([reward])))\n",
        "\n",
        "        total_loss += torch.stack(policy_loss).sum().item() + torch.stack(value_loss).sum().item()\n",
        "\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "        value_loss = torch.stack(value_loss).sum()\n",
        "\n",
        "        policy_loss.backward()\n",
        "        value_loss.backward()\n",
        "\n",
        "        actor_optimizer.step()\n",
        "        critic_optimizer.step()\n",
        "\n",
        "    return total_loss / n_epochs\n",
        "\n",
        "# Run Optuna optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Initialize the Actor-Critic networks and optimizer\n",
        "actor = QuantumPolicy(n_qubits, n_layers)\n",
        "critic = CriticNN(input_dim=4, hidden_dim=128)\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with Actor-Critic and refined reward functions\n",
        "n_epochs = 100\n",
        "gamma = 0.99  # Discount factor\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    rewards = []\n",
        "    log_probs = []\n",
        "    state_values = []\n",
        "\n",
        "    for _ in range(10):  # Example: Collect 10 trajectories\n",
        "        state = torch.tensor([[random.uniform(-1, 1) for _ in range(4)]], dtype=torch.float32)  # Random state\n",
        "        policy_output = actor(state)\n",
        "        action_prob = torch.softmax(policy_output, dim=-1)\n",
        "        action = torch.multinomial(action_prob, num_samples=1).item()  # Sample action based on probabilities\n",
        "        reward = random.uniform(-1, 1)  # Example: Refined reward function\n",
        "\n",
        "        value = critic(state).squeeze()  # Squeeze to ensure correct dimensions\n",
        "        state_values.append(value)\n",
        "        log_probs.append(torch.log(action_prob.squeeze()[action]))\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Compute discounted rewards\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted_rewards.insert(0, R)\n",
        "\n",
        "    # Normalize rewards\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-5)\n",
        "\n",
        "    # Update policy and value networks\n",
        "    actor_optimizer.zero_grad()\n",
        "    critic_optimizer.zero_grad()\n",
        "    policy_loss = []\n",
        "    value_loss = []\n",
        "    for log_prob, reward, value in zip(log_probs, discounted_rewards, state_values):\n",
        "        advantage = reward - value.item()\n",
        "        policy_loss.append(-log_prob * advantage)\n",
        "        value_loss.append(nn.MSELoss()(value, torch.tensor([reward]).squeeze()))  # Squeeze target reward\n",
        "\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    value_loss = torch.stack(value_loss).sum()\n",
        "\n",
        "    policy_loss.backward()\n",
        "    value_loss.backward()\n",
        "\n",
        "    actor_optimizer.step()\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Actor Loss: {policy_loss.item():.4f}, Critic Loss: {value_loss.item():.4f}')\n",
        "\n",
        "print(f'Final Policy Output: {policy_output}')\n",
        "\n",
        "# Example QRL environment and action selection with dynamic epsilon strategy\n",
        "class SimpleQRL:\n",
        "    def __init__(self, actor, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01):\n",
        "        self.actor = actor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Ensure proper shape\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, state.shape[-1] - 1)  # Exploration: Random action\n",
        "        else:\n",
        "            action = self.actor(state).detach().numpy()\n",
        "            return np.argmax(action)  # Exploitation: Best action based on policy\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Instantiate the environment with dynamic epsilon strategy\n",
        "env = SimpleQRL(actor, epsilon=0.1, epsilon_decay=0.99, min_epsilon=0.01)\n",
        "test_state = [0.2, -0.1, 0.5, -0.3]\n",
        "for i in range(100):\n",
        "    action = env.get_action(test_state)\n",
        "    env.decay_epsilon()\n",
        "    print(f'Selected Action: {action}, Epsilon: {env.epsilon:.4f}')"
      ],
      "metadata": {
        "id": "QDXIr64nhF-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}